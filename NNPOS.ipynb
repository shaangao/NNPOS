{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaangao/neural-net-pos-tagging/blob/main/NNPOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hglkm-tRE6UK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzFGdNNbZU7Q"
      },
      "source": [
        "## load raw datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZM9luBgVEWw"
      },
      "source": [
        "### load tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jiFB8mqaNUZl"
      },
      "outputs": [],
      "source": [
        "# func to load_dataset into a list of lists of (word, tag) tuples (each inner list is a tweet)\n",
        "\n",
        "def load_dataset(data_path):\n",
        "\n",
        "    tweets = []\n",
        "    vocab = set()\n",
        "    tags = set()\n",
        "\n",
        "    with open(data_path, 'r') as file:\n",
        "\n",
        "        tweet = []\n",
        "\n",
        "        for i, line in enumerate(file):\n",
        "\n",
        "            # if line is empty, store current tweet and start a new tweet\n",
        "            if line in ['\\n']:\n",
        "                tweets.append(tweet)\n",
        "                tweet = []\n",
        "\n",
        "            # otherwise, append new word and tag to current tweet as a tuple\n",
        "            else:\n",
        "                word, tag = line.strip('\\n').split('\\t')  # split string into word and tag\n",
        "                vocab.add(word)\n",
        "                tags.add(tag)\n",
        "                tweet.append((word, tag))\n",
        "\n",
        "    return tweets, vocab, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra3mMSlcXrXn",
        "outputId": "928e8871-b2e3-43e0-996d-7419f7341473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twpos_train: 1173, vocab_train: 4420\n",
            "twpos_dev: 327, vocab_dev: 1750\n",
            "twpos_devtest: 327, vocab_devtest: 1705\n"
          ]
        }
      ],
      "source": [
        "# load datasets\n",
        "\n",
        "twpos_train, vocab_train, tags_train = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-train.tsv')\n",
        "twpos_dev, vocab_dev, tags_dev = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-dev.tsv')\n",
        "twpos_devtest, vocab_devtest, tags_devtest = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-devtest.tsv')\n",
        "\n",
        "print(f'twpos_train: {len(twpos_train)}, vocab_train: {len(vocab_train)}\\ntwpos_dev: {len(twpos_dev)}, vocab_dev: {len(vocab_dev)}\\ntwpos_devtest: {len(twpos_devtest)}, vocab_devtest: {len(vocab_devtest)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTSepzofBxR7",
        "outputId": "2c3c2aa6-df0e-4488-907c-84aa5e132f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5991\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "# get all_vocab in train, dev, and devtest\n",
        "all_vocab = list(vocab_train.union(vocab_dev).union(vocab_devtest))\n",
        "all_vocab += ['<s>', '</s>']   # add beginning and end of sentence markers\n",
        "print(len(all_vocab))\n",
        "\n",
        "# get all_tags in train, dev, and devtest\n",
        "all_tags = list(tags_train.union(tags_dev).union(tags_devtest))\n",
        "print(len(all_tags))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load orig datasets\n",
        "\n",
        "orig_train, _, _ = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/orig-train.tsv')\n",
        "orig_dev, _, _ = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/orig-dev.tsv')\n",
        "orig_devtest, _, _ = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/orig-devtest.tsv')\n",
        "\n",
        "print(f'orig_train: {len(orig_train)}\\norig_dev: {len(orig_dev)}\\norig_devtest: {len(orig_devtest)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON8ZULTm4uMM",
        "outputId": "75bf93e4-8561-4c6d-b6c3-ab89e96215f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orig_train: 1173\n",
            "orig_dev: 327\n",
            "orig_devtest: 327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SxzpQEfVJx5"
      },
      "source": [
        "### load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKJZeoeXuJbO",
        "outputId": "92d5a60f-ac03-4397-a764-f1fab6a44170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30001 torch.Size([30001, 50])\n"
          ]
        }
      ],
      "source": [
        "# load pretrained embeddings\n",
        "\n",
        "emb_pretrained_vocab = []\n",
        "emb_pretrained = []\n",
        "\n",
        "with open('/content/drive/MyDrive/postag/data/twitter-embeddings.txt', 'r') as file:\n",
        "\n",
        "  for i, line in enumerate(file):\n",
        "\n",
        "    line_split = line.strip().split()\n",
        "\n",
        "    emb_pretrained_vocab.append(line_split[0])\n",
        "    emb_pretrained.append(list(map(float, line_split[1:])))\n",
        "\n",
        "emb_pretrained = torch.tensor(emb_pretrained)\n",
        "print(len(emb_pretrained_vocab), emb_pretrained.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIn2BFrjwN6"
      },
      "source": [
        "## encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW9XMJlvl-XT"
      },
      "source": [
        "### word & tag encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PFzfaaVZ4GcT"
      },
      "outputs": [],
      "source": [
        "# func: get idx in emb matrix given a word\n",
        "def get_word2idx(vocab_list):\n",
        "  word2idx = {}\n",
        "  for i, word in enumerate(vocab_list):\n",
        "    word2idx[word] = i\n",
        "  return word2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9-pwTd2J_KTm"
      },
      "outputs": [],
      "source": [
        "# for encoding words in context windows\n",
        "word2idx_all_vocab = get_word2idx(all_vocab)\n",
        "word2idx_emb_pretrained_vocab = get_word2idx(emb_pretrained_vocab)\n",
        "# tag2idx = get_word2idx(all_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "PxeRokjU-xQd",
        "outputId": "d62d1ea9-3170-4de0-cc52-243a4cc6cf93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# for encoding targets\n",
        "le = LabelEncoder()\n",
        "le.fit(all_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b2fJT5pmBjc"
      },
      "source": [
        "### encoded data class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, dataset:list, dataset_orig:list, word2idx:dict, tag2idx:LabelEncoder(), w:int, feature_funcs:list=None):\n",
        "\n",
        "        \"\"\"\n",
        "        wins; center_words; center_words_orig; tags; tags_encoded\n",
        "        \"\"\"\n",
        "\n",
        "        wins = []\n",
        "        center_words = []\n",
        "        tags = []\n",
        "        center_words_orig = []   # from the orig-* files\n",
        "\n",
        "        # encode context window and center word featuress\n",
        "        for tweet, tweet_orig in zip(dataset, dataset_orig):\n",
        "\n",
        "            # process every center word in each tweet\n",
        "            for i, (word, tag) in enumerate(tweet):\n",
        "\n",
        "                # center word for curr obs\n",
        "                center_words.append(word)\n",
        "\n",
        "                # orig center word for curr obs\n",
        "                center_words_orig.append(tweet_orig[i][0])\n",
        "\n",
        "                # target of curr obs\n",
        "                tags.append(tag)\n",
        "\n",
        "                # construct win: idx for words in context window\n",
        "                win = []\n",
        "                for i in range(i-w, i+w+1):\n",
        "                    if i < 0:   # if before fist token\n",
        "                        try: win.append(word2idx['<s>'])\n",
        "                        except: win.append(word2idx['</s>'])   # if '<s>' not in emb vocab, use emb for '</s>'\n",
        "                    elif i > len(tweet)-1:    # if after last token\n",
        "                        win.append(word2idx['</s>'])\n",
        "                    else:\n",
        "                        try: win.append(word2idx[tweet[i][0]])\n",
        "                        except: win.append(word2idx['UUUNKKK'])  # use emb for unknown words\n",
        "                wins.append(win)\n",
        "\n",
        "        # encode all target tags\n",
        "        tags_encoded = tag2idx.transform(tags)\n",
        "\n",
        "        # data type\n",
        "        wins = torch.tensor(wins)\n",
        "        center_words = np.array(center_words)\n",
        "        center_words_orig = np.array(center_words_orig)\n",
        "        tags_encoded = torch.tensor(tags_encoded)\n",
        "        tags = np.array(tags)\n",
        "\n",
        "        # construct features from center_words_orig using feature_funcs\n",
        "        if feature_funcs is not None:\n",
        "            features = np.column_stack([np.vectorize(func)(center_words_orig) for func in feature_funcs])\n",
        "            wins = torch.cat((wins, torch.tensor(features)), dim=1)\n",
        "\n",
        "        # set attributes\n",
        "        self.wins = wins\n",
        "        self.center_words = center_words\n",
        "        self.center_words_orig = center_words_orig\n",
        "        self.tags_encoded = tags_encoded\n",
        "        self.tags = tags\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wins)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wins[idx], self.tags_encoded[idx]"
      ],
      "metadata": {
        "id": "KwNLFy5Y-UvM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbTzoiLQnDxw"
      },
      "source": [
        "### instantiate encoded datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "554uLQLbFw_x"
      },
      "outputs": [],
      "source": [
        "# encode datasets\n",
        "\n",
        "\n",
        "# w = 0, all vocab encoding\n",
        "train_w0_allvocab = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_all_vocab, tag2idx=le, w=0, feature_funcs=None)\n",
        "dev_w0_allvocab = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=0, feature_funcs=None)\n",
        "devtest_w0_allvocab = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=0, feature_funcs=None)\n",
        "\n",
        "# w = 1, all vocab encoding\n",
        "train_w1_allvocab = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_all_vocab, tag2idx=le, w=1, feature_funcs=None)\n",
        "dev_w1_allvocab = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=1, feature_funcs=None)\n",
        "devtest_w1_allvocab = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=1, feature_funcs=None)\n",
        "\n",
        "\n",
        "# w = 0, pretrained 30k vocab encoding\n",
        "train_w0_30k = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0, feature_funcs=None)\n",
        "dev_w0_30k = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0, feature_funcs=None)\n",
        "devtest_w0_30k = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0, feature_funcs=None)\n",
        "\n",
        "# w = 1, pretrained 30k vocab encoding\n",
        "train_w1_30k = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1, feature_funcs=None)\n",
        "dev_w1_30k = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1, feature_funcs=None)\n",
        "devtest_w1_30k = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1, feature_funcs=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLiCP5wG5Wu"
      },
      "source": [
        "## 1.1 baseline neural network tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2lqvEXdNdWI"
      },
      "source": [
        "### model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "m16SxaOYNNr7"
      },
      "outputs": [],
      "source": [
        "# references:\n",
        "# - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "# - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "# - https://discuss.pytorch.org/t/how-to-create-mlp-model-with-arbitrary-number-of-hidden-layers/13124/2\n",
        "# - https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
        "# - https://machinelearningmastery.com/activation-functions-in-pytorch/\n",
        "\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, w, vocab_size, emb_dim, nfeatures, layer_sizes:list, layer_acts:list, pretrained_emb=None, emb_freeze=False):\n",
        "\n",
        "        # call parent constructor\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "\n",
        "        # set initial embeddings\n",
        "        if pretrained_emb is not None:\n",
        "            self.emb = nn.Embedding.from_pretrained(pretrained_emb, freeze=emb_freeze)\n",
        "        else:   # randomly init embeddings\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "            self.emb.weight.data.uniform_(-0.01, 0.01)\n",
        "\n",
        "        # set embeddings' dimensionality\n",
        "        self.emb_dim = self.emb.weight.shape[1]\n",
        "\n",
        "        # set total num of words in win\n",
        "        self.w = 1 + 2 * w\n",
        "\n",
        "        # set input layer dimensionality\n",
        "        in_size = self.emb_dim * self.w + nfeatures\n",
        "\n",
        "        # construct layers (last layer is output layer)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, layer_size in enumerate(layer_sizes):\n",
        "            if i == 0:\n",
        "                layer = nn.Linear(in_size, layer_size)\n",
        "                layer.weight.data.uniform_(-0.01, 0.01)\n",
        "                layer.bias.data.zero_()\n",
        "                self.layers.append(layer)\n",
        "                # self.layers.append(nn.Linear(in_size, layer_size))\n",
        "            else:\n",
        "                layer = nn.Linear(layer_sizes[i-1], layer_size)\n",
        "                layer.weight.data.uniform_(-0.01, 0.01)\n",
        "                layer.bias.data.zero_()\n",
        "                self.layers.append(layer)\n",
        "                # self.layers.append(nn.Linear(layer_sizes[i-1], layer_size))\n",
        "\n",
        "        # set each layer's activation function\n",
        "        self.layer_acts = layer_acts\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # print('before', x.shape, x[:, :self.w], x[:, self.w:])\n",
        "\n",
        "        # retrieve context word embeddings and concat horizontally; concat additional features to the right\n",
        "        x = torch.cat(\n",
        "            (\n",
        "                self.emb(x[:, :self.w]).view((x.shape[0], -1)),   # word embeddings\n",
        "                x[:, self.w:]     # additional features\n",
        "            ),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # print('after', x.shape)\n",
        "\n",
        "        # forward pass\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = self.layer_acts[i](x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train and eval func"
      ],
      "metadata": {
        "id": "fe_-lvTc469g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run one epoch of training\n",
        "\n",
        "def train1epoch(model, optimizer, criterion, train_dataloader):\n",
        "\n",
        "    # turn on training mode\n",
        "    model.train()\n",
        "\n",
        "    # reset epoch_loss tracker\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # iterate through mini-batches\n",
        "    for xtrain_batch, ytrain_batch in train_dataloader:\n",
        "\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        output = model(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "yyMJ77n-3S9O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "\n",
        "def eval(model, eval_data):\n",
        "\n",
        "    # turn on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # turn off gradient calc to reduce memory consumption\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # get model predictions on eval_data\n",
        "        ypred = torch.argmax(model(eval_data.wins), dim=1)\n",
        "\n",
        "        # count correct predictions\n",
        "        ycorrect = torch.sum(torch.eq(ypred, eval_data.tags_encoded)).item()\n",
        "\n",
        "        # total num of obs in eval_data\n",
        "        ytotal = len(eval_data.tags_encoded)\n",
        "\n",
        "        # compute accuracy\n",
        "        yaccu = ycorrect / ytotal\n",
        "\n",
        "    print(f'  accuracy: {yaccu}')\n",
        "    return yaccu"
      ],
      "metadata": {
        "id": "lAseCrvg-oc8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train & eval wrapper"
      ],
      "metadata": {
        "id": "0AXYBgW1MhLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper for train & eval\n",
        "# reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "def main_process(model, name, optimizer, criterion, train_data, batch_size, shuffle, val_data, test_data, max_epochs=10, early_stopping=3):\n",
        "\n",
        "\n",
        "    # create batched iterator for train_data\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "    # initialize vars: track metrics\n",
        "    epoch_losses = []\n",
        "    train_evals = []\n",
        "    dev_evals = []\n",
        "\n",
        "    # initialize vars: track best model\n",
        "    best_dev_eval = 0\n",
        "    best_model_epoch = -1\n",
        "\n",
        "    # train and eval\n",
        "    for epoch in range(max_epochs):\n",
        "\n",
        "        print(f'epoch {epoch+1}')\n",
        "\n",
        "        # train\n",
        "        epoch_loss = train1epoch(\n",
        "                          model=model,\n",
        "                          optimizer=optimizer,\n",
        "                          criterion=criterion,\n",
        "                          train_dataloader=train_dataloader\n",
        "                    )\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "        # eval on training set\n",
        "        train_eval = eval(model=model, eval_data=train_data)\n",
        "        train_evals.append(train_eval)\n",
        "\n",
        "        # eval on dev set\n",
        "        dev_eval = eval(model=model, eval_data=val_data)\n",
        "        dev_evals.append(dev_eval)\n",
        "\n",
        "        # update best model based on dev eval\n",
        "        if dev_eval > best_dev_eval:\n",
        "\n",
        "            # save state_dict of best model so far\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/postag/models/'+name+'_best.pth.tar')\n",
        "\n",
        "            # update which epoch best_model is from\n",
        "            best_model_epoch = epoch\n",
        "\n",
        "            # update best_dev_accu\n",
        "            best_dev_eval = dev_eval\n",
        "\n",
        "        print(f'  best model from epoch {best_model_epoch+1}')\n",
        "\n",
        "        # early stopping based on dev eval\n",
        "        if early_stopping is not None:\n",
        "            if epoch - best_model_epoch >= early_stopping:\n",
        "                print('========= EARLY STOPPING =========')\n",
        "                break\n",
        "\n",
        "\n",
        "    # load state_dict of best model (modifies input model in place)\n",
        "    print(f'load best model')\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/postag/models/'+name+'_best.pth.tar'))\n",
        "\n",
        "    # eval best model on devtest set\n",
        "    print(f'eval best model on devtest')\n",
        "    devtest_eval = eval(model=model, eval_data=test_data)\n",
        "\n",
        "\n",
        "    return epoch_losses, train_evals, dev_evals, devtest_eval\n"
      ],
      "metadata": {
        "id": "yM4nxqEAOi9X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run model: w=0, all vocab"
      ],
      "metadata": {
        "id": "NNl16tuaZ8MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=0, all vocab (random init)\n",
        "tagger_w0 = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50, nfeatures=0,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                          model=tagger_w0,\n",
        "                                          name='tagger_w0',  # file name used for using checkpoint\n",
        "                                          optimizer=sgd,\n",
        "                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                          train_data=train_w0_allvocab,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          val_data=dev_w0_allvocab,\n",
        "                                          test_data=devtest_w0_allvocab,\n",
        "                                          max_epochs=20,\n",
        "                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7ODC7aB86rl",
        "outputId": "50dadd0d-794c-4496-e275-b602e2b9a3ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46360.52154862881\n",
            "  accuracy: 0.13736135434909516\n",
            "  accuracy: 0.13690105787181084\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 33964.24647471821\n",
            "  accuracy: 0.5658493870402802\n",
            "  accuracy: 0.5328769964737606\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 22743.718198784278\n",
            "  accuracy: 0.7480443666082895\n",
            "  accuracy: 0.7033810412777433\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 14901.21043849986\n",
            "  accuracy: 0.8575014594279042\n",
            "  accuracy: 0.750674133997096\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 11242.088181002444\n",
            "  accuracy: 0.896322241681261\n",
            "  accuracy: 0.7598008711885501\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 8556.753457576851\n",
            "  accuracy: 0.9193228254524226\n",
            "  accuracy: 0.7728686994399502\n",
            "  best model from epoch 6\n",
            "epoch 7\n",
            "  epoch loss: 6902.9516340345435\n",
            "  accuracy: 0.9268534734384122\n",
            "  accuracy: 0.7768097904998963\n",
            "  best model from epoch 7\n",
            "epoch 8\n",
            "  epoch loss: 6002.3917924894195\n",
            "  accuracy: 0.9207822533566842\n",
            "  accuracy: 0.770172163451566\n",
            "  best model from epoch 7\n",
            "epoch 9\n",
            "  epoch loss: 5477.893790991569\n",
            "  accuracy: 0.9256275539988325\n",
            "  accuracy: 0.7778469197261979\n",
            "  best model from epoch 9\n",
            "epoch 10\n",
            "  epoch loss: 5132.106349430047\n",
            "  accuracy: 0.9288382953882078\n",
            "  accuracy: 0.7782617714167185\n",
            "  best model from epoch 10\n",
            "epoch 11\n",
            "  epoch loss: 4793.107766985544\n",
            "  accuracy: 0.9279042615294805\n",
            "  accuracy: 0.776602364654636\n",
            "  best model from epoch 10\n",
            "epoch 12\n",
            "  epoch loss: 4591.493362655194\n",
            "  accuracy: 0.927729130180969\n",
            "  accuracy: 0.7741132545115121\n",
            "  best model from epoch 10\n",
            "epoch 13\n",
            "  epoch loss: 4470.116119947081\n",
            "  accuracy: 0.9201401050788091\n",
            "  accuracy: 0.7687201825347438\n",
            "  best model from epoch 10\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.7909032118991162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 0, the best taggging accuracy on DEV is 77.83% from epoch 10; this best model has a tagging accuracy of 79.09% on DEVTEST.   \n",
        "(In the cell outputs above, the first accuracy score in each epoch is the accuracy on TRAIN, and the second accuracy score in each epoch is the accuracy on DEV. Same below.)"
      ],
      "metadata": {
        "id": "LZLWcBugUhUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run model: w=1, all vocab"
      ],
      "metadata": {
        "id": "G9WvHFmLTzWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=1, all vocab (random init)\n",
        "tagger_w1 = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50, nfeatures=0,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                                          model=tagger_w1,\n",
        "                                                          name='tagger_w1',  # file name used for using checkpoint\n",
        "                                                          optimizer=sgd,\n",
        "                                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                                          train_data=train_w1_allvocab,\n",
        "                                                          batch_size=1,\n",
        "                                                          shuffle=True,\n",
        "                                                          val_data=dev_w1_allvocab,\n",
        "                                                          test_data=devtest_w1_allvocab,\n",
        "                                                          max_epochs=20,\n",
        "                                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZSZXPfV-daL",
        "outputId": "cd1fd160-2eae-4a5e-88f5-778fa7239527"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46358.82570374012\n",
            "  accuracy: 0.1514302393461763\n",
            "  accuracy: 0.1557768097904999\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 29023.68808175315\n",
            "  accuracy: 0.7378867483946293\n",
            "  accuracy: 0.701929060360921\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 12888.414400380258\n",
            "  accuracy: 0.8904261529480444\n",
            "  accuracy: 0.7826177141671853\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 7965.6317956046805\n",
            "  accuracy: 0.9298890834792761\n",
            "  accuracy: 0.800663762704833\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 6233.374623492888\n",
            "  accuracy: 0.9455341506129598\n",
            "  accuracy: 0.8039825762289982\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 4933.45225395245\n",
            "  accuracy: 0.9447752481027437\n",
            "  accuracy: 0.7919518772038996\n",
            "  best model from epoch 5\n",
            "epoch 7\n",
            "  epoch loss: 3884.0792413342547\n",
            "  accuracy: 0.960128429655575\n",
            "  accuracy: 0.7979672267164488\n",
            "  best model from epoch 5\n",
            "epoch 8\n",
            "  epoch loss: 3049.7473814702535\n",
            "  accuracy: 0.9688266199649738\n",
            "  accuracy: 0.8025305953121759\n",
            "  best model from epoch 5\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.810303944815693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 1, the best taggging accuracy on DEV is 80.40% from epoch 5; this best model has a tagging accuracy of 81.03% on DEVTEST.   \n",
        "(In the cell outputs above, the first accuracy score in each epoch is the accuracy on TRAIN, and the second accuracy score in each epoch is the accuracy on DEV.)"
      ],
      "metadata": {
        "id": "sX6vUY0UVJWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 feature engineering"
      ],
      "metadata": {
        "id": "x4K_RderVlaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### examine model errors"
      ],
      "metadata": {
        "id": "qJxXFWfZo0Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best tagger so far\n",
        "tagger_w1 = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50, nfeatures=0,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "tagger_w1.load_state_dict(torch.load('/content/drive/MyDrive/postag/models/tagger_w1_best.pth.tar'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcbeFzF3YEDN",
        "outputId": "201d387d-0423-4d3f-bd5a-4ec7d19eb9bd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the errors the best tagger so far made on *DEV*\n",
        "\n",
        "# get a mask for correct predictions\n",
        "ycorrect_mask = torch.eq(\n",
        "    torch.argmax(tagger_w1(dev_w1_allvocab.wins), dim=1),  # pred\n",
        "    dev_w1_allvocab.tags_encoded  # true\n",
        ")\n",
        "\n",
        "# get center words with wrong pred\n",
        "yerror = dev_w1_allvocab.center_words_orig[~ycorrect_mask]\n",
        "print([e for e in yerror])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLGGBqhX7D7s",
        "outputId": "1360f02a-86a8-44d3-fd81-95e6c3948706"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@ciaranyree', 'it', 'was', 'on', ',', 'one', 'of', 'the', 'players', 'and', 'his', 'wife', 'own', 'burger', 'RT', '@TheRealQuailman', ':', 'Currently', 'laughing', 'at', '.', 'RT', '@HollywoodOompa', ':', 'Sat', '6', 'ill', 'be', 'at', 'Nashville', 'center', 'stage', 'for', 'the', 'ice', 'kream', 'for', '<<', \"it's\", 'the', 'music', 'center', 'center', 'You', \"don't\", 'know', 'my', 'struggle', 'Wind', '2.0', 'mph', '.', 'Barometer', '29.676', ',', 'Rising', '.', 'Temperature', '56.3', '.', 'Rain', 'today', '0.00', '.', '45%', '@ShiversTheNinja', 'forgive', 'me', 'for', 'blowing', 'up', 'your', 'comment', 'section', '.', 'New', ':', 'How', 'CAN', 'you', 'mend', 'a', 'broken', '?', 'Really', '?', 'Please', '?:', 'http://bit.ly/9RgG9L', '@justinbieber', 'can', 'u', 'follow', 'me', 'please', '???', 'it', 'mean', 'the', 'world', 'to', 'me', '!!', ':)', '@JoycieeLovesUuu', '=)', 'Lls', '@carolyncallahan', 'I', 'knew', 'it', 'last', 'night', ',', 'but', \"didn't\", 'bother', 'calling', 'Shawn', 'because', \"I'd\", 'just', 'be', 'working', 'on', 'it', 'this', 'morning', 'w/', 'same', 'info', '.', 'Senate', '#ArtsGrades', 'are', 'in', '!', 'See', 'who', 'passed', 'and', 'who', 'made', 'the', 'Dirty', '.', '#arts', 'http://t.co/BAh2iUL', 'via', '@ArtsActionFund', 'RT', '@guarnaschelli', ':', 'I', 'want', 'to', 'at', 'a', 'in', 'a', 'diner', 'and', 'watch', 'the', 'news', 'and', 'out', 'over', 'some', 'fresh', 'pancakes', ',', 'with', 'rum', ',', 'cri', '...', '29p', '11r', 'Pal', 'went', 'da', 'fuck', 'off', '@comicsguy024', 'I', \"don't\", 'use', 'due', 'to', 'the', 'lack', 'of', 'a', 'good', 'Twitter', 'extension', '(', 'and', 'extension', 'behavior', 'in', 'general)', '.', 'Also', ',', 'it', 'looks', 'weird', 'So', \"who's\", 'going', 'to', 'the', 'Ethernet', 'Expo', 'next', 'week', 'in', '?', \"I'm\", 'addicted', 'to', 'that', '.', 'X', ')', 'RT', '@DarCoxaj', ':', 'TETRIS', '!', '(:', '\"', '@gcouros', 'I', 'see', 'that', ',', 'regretfully', 'I', 'was', 'tied', 'up', ',', 'Physed', 'at', '-', 'have', 'a', 'good', 'one', 'Costume', 'ready', '!', 'Where', 'to', 'go', 'for', 'Halloween', 'on', 'and', 'sat', '...', 'Thinking', 'pyramid', 'on', 'sat', '...', 'Confirmed', 'no', 'phone', ',', 'now', 'I', 'have', 'to', 'go', 'to', 'school', 'tomorrow', 'so', 'I', \"don't\", 'get', 'to', 'see', '@taybby21', 'till', 'the', 'weekend', 'or', '.', '/', ':', \"Mom's\", '.', '10/27-', 'Make', 'sure', 'to', 'get', 'your', 'IR', 'uploaded', '...', 'AND', 'dress', 'up', 'for', 'Halloween', 'on', 'Friday', '!!', '\"', 'Because', 'if', 'seeing', 'is', 'believing', ',', 'then', 'that', 'we', 'have', 'lost', 'our', '...\"', '#fb', '@GUUuh_POwEr', 'u', 'from', 'Nashville', '?', 'RT', '@Gromit01', ':', 'Get', 'into', 'an', 'argument', 'with', 'your', 'tattooist', '?', 'What', 'could', 'possibly', 'go', 'wrong', '?', 'http://tinyurl.com/2e2z82n', 'wow', 'Jay-Z', 'Responds', 'To', 'Beyonce', 'Pregnancy', '|', 'The', 'http://bit.ly/aMjMsZ', '@MyName_IsAndre', 'lol', 'boy', 'shut', 'up', '!', 'I', 'wonder', 'if', 'you', 'realize', 'we', 'were', 'YOU', '.', 'RT', '@xerxeslux', ':', 'Charice', '!!', '#breakoutmusicartist', '#PeoplesChoice', '@OfficialCharice', 'http://bit.ly/aDTggc', '#FOLLOW', '---->', '@_FashionCRAZED', '@MisGenevieve', '@LauraAshley913', '@NishaGotEmLoced', '@iT0NGUEuSQUIRT', '@SoLacedUp_', '@Ggs_PrettyInked', '@Twon_GotStaCCs', 'RT', '@stevedupe', ':', 'Holy', 'crap', '-', 'going', 'to', 'war', 'http://www.reuters.com/article/idUSN2618043220101027', 'Ignite', 'to', 'Write', ':', 'This', 'Photo', 'Needs', 'a', 'Title', '!:', 'This', 'photograph', 'certainly', 'needs', 'a', 'title', '!', 'Can', 'you', 'give', 'it', 'one', '?', 'It', 'se', '...', 'http://bit.ly/bz8n9w', '@thehunternorris', 'nah', 'man', 'your', 'a', '#biebro', \"don't\", 'think', 'I', \"didn't\", 'save', 'that', 'phone', 'call', '!', ';)', \"What's\", 'scarier', 'than', 'fake', 'blood', ',', 'guts', 'and', '?', 'How', 'much', 'they', 'can', 'cost', ':', 'S', 'So', 'try', 'these', 'homemade', 'solutions', '!', 'http://tinyurl.com/27gdfc4', 'RT', '@CristalFlo', ':', 'I', 'know', 'it', 'gets', 'repetitive', 'but', '.....', 'I', 'love', 'my', 'life', '76', '??', 'AMERICA', ',', 'FUCK', 'YEAH', '!', '@HuffingtonPost', 'Awesome', 'Old', 'Man', 'Makes', 'A', 'With', 'A', 'Gun', '(', ')', 'http://huff.to/cgyuKq', '@PleazeBelieveMe', 'lmao', 'u', 'should', 'see', 'my', 'hand', 'motions', 'We', 'went', '2', 'long', 'without', 'talkin', 'bout', 'some', 'shit', 'I', \"don't\", 'care', '2', 'discuss', '.', 'I', 'guess', 'its', 'that', 'time', '.', 'The', 'peace', 'was', 'good', 'while', 'it', 'lasted', '*', 'sighs', 'Am', 'I', 'ur', 'one', 'and', 'only', '?', 'Am', 'I', 'the', 'reason', 'you', 'breathe', ',', 'or', 'am', 'I', 'the', 'reason', 'you', '#cry', '?', '#np', \"can't\", 'get', '@brutha', '♥', 'the', 'blog', 'updates', '...', 'http://fb.me/KCCi1JuT', 'coverage', 'of', 'and', 'Friends', 'Concert', '...', 'I', 'love', 'him', 'and', 'he', 'love', 'me', '...', 'aint', 'nuthin', 'u', 'can', 'tell', 'us', '...', 'u', 'can', 'try', 'all', 'u', 'want', 'to', 'this', 'bond', 'too', 'strong', '...', 'so', 'imma', 'n…', 'http://lnk.ms/DcpY6', '@jesseecahh', 'him', ',', 'lmfao', '-', 'But', 'of', 'his', 'fat', 'ass', 'wants', 'treats', 'so', 'either', 'way', 'I', 'had', 'to', 'get', 'up', '.', '@theGypsy', 'my', 'comment', '(', 'currently', 'awaiting', 'moderation', ')', 'probably', 'chase', 'them', 'away', ';)', 'WAIT', 'A', 'SECOND', \"IT'S\", 'ON', 'MY', 'PHONE', 'HAMMER', 'U', 'ARE', 'MAD', 'GAY', 'AND', 'U', 'NOT', 'DO', 'ANYTHING', 'u', 'mad', 'I', 'am', 'illuminate', 'I', 'am', 'king', 'Augustus', 'say', 'it', 'to', 'me', 'RT', '@ObserverDallas', ':', 'with', 'is', 'playing', 'at', 'this', '!', 'Want', 'to', 'be', 'a', 'personal', 'guest', 'of', 'Observer', '?', 'RT', 'to', 'win', 't', '...', 'I', 'love', 'my', 'she', 'lettin', 'me', 'out', 'just', 'in', 'time', 'for', 'my', '#Colts', 'game', 'siiiiiced', 'RT', '@TerrenceJ106', ':', 'I', 'can', 'say', 'that', 'tonights', '106', 'is', 'gonna', 'be', 'a', '!', '@NICKIMINAJ', '&', 'Janet', 'on', 'the', 'same', 'show', '?', '#goingtoworkontime', '!', 'World', 'Cup', \"soccer's\", 'psychic', 'octopus', 'Paul', 'dies', 'in', 'http://bit.ly/d6qTGc', 'Ring', '!', '#lakers', 'Slightly', 'Stoopid', '@', 'ACL', 'with', 'Slacker', 'part', '2', ':', 'http://fb.me/Ai9Qfi5J', 'first', 'game', 'on', 'the', '27th', 'vs', '.', 'Houston', 'This', 'is', 'startin', 'to', 'get', 'borin', '...', '@Joigadoi', 'thanks', ':D', 'East', 'Bay', '–', '10/26', 'http://nblo.gs/9H1z8', 'Happy', 'Year', 'of', '!', 'What', 'better', 'way', 'to', 'than', 'in', 'to', \"CropLife's\", 'Biodiversity', 'World', 'Tour', '.', '#BWT2010', 'Finds', 'And', 'And', 'Takes', 'Social', ':', '“People', 'like', 'stumbling', 'videos', 'more', 'than', 'web', '...', 'http://bit.ly/c4gny7', \"I'm\", 'not', 'a', 'boy', '-_-', '@WilliefknUnique', 'hahah', 'well', 'that', 'a', 'diff', 'story', 'then', ':P', 'lmao', 'he', 'doesnt', 'wanna', 'talk', 'to', 'ya', '.', 'hes', 'talkin', 'to', 'me', '!', ';)', 'We', 'has', 'real', 'internets', 'at', 'laaaaaaaaast', '!', '{{', 'Is', 'only', 'in', '@s', 'while', 'watching', 'Get', 'Him', 'to', 'the', '!', '}}', 'RT', '@liltunechi', ':', '8', 'days', 'until', 'they', '#FreeWeezy', '.', 'Shoutout', '#FreeWeezy', 'all', 'day', 'everyday', 'until', \"he's\", 'home', '!!', 'RT', '@SugaHunniIceTea', ':', 'But', \"I'm\", 'going', 'to', 'praise', 'at', 'my', 'lowest', 'point', '.', 'Amen', 'Super', 'for', 'our', 'Halloweeen', 'tomorrowwww', ':)', 'TRICK', 'OR', 'TREAT', '!', '@SkyHearDOTcom', 'Your', 'local', 'and', \"that's\", 'cool', ',', 'hit', 'me', 'up', 'direct', 'would', 'like', 'to', 'discuss', 'where', 'this', 'is', 'going', '...', 'Now', 'I', 'to', 'go', 'home', 'and', 'make', 'nachos', 'with', 'this', 'probably', 'bad', 'for', 'me', 'cheese', 'sauce', '.', '#mmmm', 'Announce', 'Partnership', 'with', 'USGN', 'http://ow.ly/19EQDn', '@monaFCKN_lisa', '#oneword', ':', 'LAZYASFUCK', 'lol', 'RT', '@yungboikortny', 'Ray', 'Allen', 'Look', 'Just', 'Like', 'My', 'Dad', '!', 'Lol', '<==', '#FlagOnThePlay', '@JeffroMusic', '@arlene_chang', 'she', 'cant', 'Cus', 'she', 'probably', 'the', 'same', 'size', 'as', 'the', 'bop', 'haha', 'well', 'that', 'strange', '!', 'hahahahhaa', '.', 'airlines', 'from', 'sta', 'http://www.bestcouponfor.com/coupon/266684.aspx', 'Support', 'Pray', 'For', 'Indonesia', ',', 'add', 'a', 'to', 'your', 'now', '!', '-', 'http://twb.ly/cHMmwJ', '#PrayForIndonesia', '@PimpThoughts', 'the', 'invite', 'was', 'actually', '2', 'lure', \"'\", 'em', 'in2', 'the', 'den', 'of', 'lions', '.', 'She', 'assured', 'me', \"they'd\", 'bring', 'napkins', '2', 'wipe', 'away', 'tears', 'frm', '#roastin', '@ayoooitsRICA', '(', ')', 'a', 'person', 'from', 'seeing', 'the', 'the', 'in', 'others', '.', 'I', 'think', 'I', 'get', 'fever', '.', 'Geez', '.', 'RT', '@MansionOct29', ':', 'BLACK', 'PARTY', 'HITS', '98', 'TODOAY', '!', '7046614311', 'FOR', 'INFO', '@Danithepoet', '@DanniGyrl1', '@daRealCityboi', '@Darling_IamWarr', '@daveknowspdx', 'We', 'are', 'making', 'a', 'difference', 'already', ':', 'http://bit.ly/c8fLnr', 'RT', '@djunique23', ':', '@MRMcGill85', 'LOL', 'HE', 'CANT', 'WIN', 'A', 'GAME', 'BY', 'HIS', \"SELF(that's\", 'why', 'he', 'got', 'the', 'other', '2', '!!', ')(', 'RT', '@Pixer23', 'Fun', 'times', 'http://twitpic.com/317itr', '<', 'love', 'this', 'haha', 'heelllooo', 'Just', 'out', \"who's\", 'been', 'callin', 'me', 'private', 'n', 'hangin', 'up', ',', 'and', 'all', 'i', 'have', 'to', 'say', 'is', '...', 'get', 'the', 'fuck', 'outta', 'here', '!', 'lmao', '@shaeken', 'what', 'me', '.....', 'Cry', 'cry', 'What', 'you', 'see', 'is', 'what', 'you', 'get', 'Part', 'Builder', 'I', ':', 'Define', 'http://bit.ly/bZyU80', 'RT', '@sultrysole', ':', 'the', 'way', 'some', 'guys', 'talk', 'girls', 'makes', 'me', 'cringe', 'How', 'does', 'Morrison', 'have', 'a', 'ring', ',', 'and', 'Miller', \"doesn't\", 'have', 'one', '?', '#mysteriesofHOOPS', '|', ',', 'instant', 'screenshots', 'and', 'screencasts', ',', 'home', 'http://t.co/c6alsyO', 'via', '@AddThis', \"╚✰ѕнoυтoυт'ѕ™✰╝>>>>\", '@allilove_SODMG', 'Animation', 'of', 'yesterdays', 'historic', '.', 'http://severeplains.com/images/102610_surfacelow.html', '@chewthis_bash', 'lol', 'how', '?', \"I'm\", 'never', 'small', 'minded', 'RT', '@CynthiaBoS', ':', 'Grrrrrrrr', '!', '/', 'Guau', '!', 'I', 'love', 'dwayne', 'wade', '!!!!', 'I', 'wanna', 'marry', 'him', '!!!', ';)', 'RT', '@KevinDing', ':', 'Shannon', 'is', 'tonight', ':', 'points', 'in', '17', 'minutes', ',', '6', 'of', '8', 'shooting', ',', '4', 'of', '5', 'from', 'deep', '--', 'plus', 'some', 'key', '.', 'RT', '@ProSeverusSnape', ':', '“All', 'that', 'glitters', 'is', 'not', 'True', '.', 'Sometimes', 'it’s', 'pretending', 'to', 'be', 'vampires', '.', 'in', 'point', ',', 'Edward', '.', 'What', '...', \"I've\", 'had', 'a', 'few', 'requests', 'that', 'I', 're-enable', 'on', 'on', 'my', '.', 'Looking', 'for', 'rigs', 'for', 'my', '#7D', 'If', 'any1', 'can', 'point', 'me', 'in', 'the', 'thatd', 'be', 'great', '!', '#Video', '#Ikan', '#RedRockMicro', '#Zacuto', '#IDC', '#eBay', 'RT', '@zwriter', ':', '@dustinlong', 'USAC', 'pledged', '10¢', 'for', 'Shane', 'Recovery', 'Fund', 'for', 'every', 'new', 'FB', 'fan-', 'link', ':', 'http://on.fb.me/bnqidV', '.', 'Lets', 'get', '...', 'People', 'always', 'what', 'they', \"don't\", 'know', '.', 'Back', 'to', 'real', 'life', ':', 'picking', 'up', 'Camp', 'Kern', 'fundraiser', 'pizzas', 'tonight', ',', 'hoping', 'to', 'start', 'on', 'bathroom', 'tile', 'tomorrow', 'and', 'laundry', '...', 'always', 'laundry', '..', '@DjDSTRONG', 'I', 'would', 'of', 'swore', 'u', 'said', 'that', 'yesterday', ':/', 'lol', 'Before', 'I', 'get', 'started', 'on', 'this', 'post', 'me', 'make', 'one', 'thing', 'absolutely', 'clear', '.', 'I', 'am', 'a', '.', 'My', 'is', 'to', 'develo', '-', 'http://bit.ly/b50fIF', 'RT', '@RealSkipBayless', ':', 'Are', 'you', 'SURE', 'this', 'was', 'Big', '3', 'vs', '.', 'Big', '3', '?', 'has', 'played', 'up', 'in', 'Toronto', ',', 'away', 'from', ',', 'pressure', '.', 'This', 'stage', 'loo', '...', 'is', 'already', 'on', 'the', '106', 'top', 'ten', 'countdown', ',', 'it', 'really', 'is', 'mind', 'blowing', '.', '@justinbieber', 'i', 'was', 'going', 'to', 'go', 'to', 'your', 'concert', 'in', 'and', 'i', 'couldnt', 'because', 'i', 'couldnt', 'get', 'a', ':((', 'i', 'wish', 'went', '.', 'RT', '@A_Rrod', 'WARNER', 'CABLE', 'WHAT', 'THE', 'FUCK', 'IS', 'THE', 'FUCKIN', 'PROBLEM', 'It', 'needs', 'to', 'be', 'wrestling', 'season', 'so', 'I', 'can', 'concentrate', 'on', 'other', 'than', 'this', '.', 'Real', 'niqqa', 'stfu', 'RT', '@eklecticx', ':', 'Chocolate', 'Mod', 'Beatle', 'Boots', 'by', 'eklecticxplosion', 'on', 'http://bit.ly/c95nZY', '#Etsy', '@BeliebinKPerry', 'what', 'is', '?', '@Miz_Kellie', 'wat', 'cha', 'doin', 'flirty', 'aprons', \"mother's\", '40%', 'off', 'sale', ',', 'code', 'fa-4110', '.', 'http://www.bestcouponfor.com/coupon/251486.aspx', 'Teaching', '.', 'Fuck', 'you', 'America', '...', 'stfu', 'IS', 'NOT', 'silent', '...', 'and', 'we', 'still', 'up', 'NIGGA', 'WHAT', '!!!', '@MisguidedGhost', 'me', '=)', '@DrakeswifeTiffy', 'yea', 'you', '!', ':)', 'I', \"ain't\", 'got', 'time', '..', 'bye', 'bye', '.', 'A', 'day', 'without', 'you', 'is', 'like', 'a', 'Year', 'Without', 'Rain', '<3', '@irenitapetty4', 'First', 'concert', '?', 'Ohmygosh', 'awww', '<3', '.', 'How', 'it', 'was', '?', ':D', 'RT', '@trini87', ':', 'I', \"don't\", 'see', 'any', 'fans', 'on', 'my', '...', 'all', 'I', 'see', 'is', '#teamlakers', '...', 'is', '...', 'do', 'rocket', 'fans', 'exist', '?', 'MC', 'Hammer', 'beefing', 'with', 'Jay-z', '!!??', 'Lmao', 'wait', 'no', '\"', 'King', 'Hammer', '\"', 'RT', '@kpopidol', ':', 'True', ':', \"they're\", 'who', 'accept', 'any', \"member's\", 'decision', '!!!', 'if', 'u', 'are', 'true', 'shawol', '!!!', \"i'm\", 'supporting', 'them', 'even', 'i', 'fell', 'a', 'bit', '...', 'RT', '@grungereport', ':', 'GrungeReport', ':', 'DAVE', '&', 'NOVOSELIC', 'OF', 'CONFIRMED', 'TO', 'REUNITE', 'FOR', 'A', 'SONG', 'ON', 'NEW', 'FOO', 'ALBUM', 'http', ':/', '...', 'Love', '@GansevoortPark', '!', 'Thanks', 'for', 'taking', 'awesome', 'of', 'our', ':)', '@anidasabrina', '!!', ':)', \"Today's\", 'MetroTube', ':', 'An', 'oddly', 'touching', 'electro', 'to', 'celebrity', 'mugshots', '—', 'it', 'almost', 'makes', 'you', 'feel', 'bad', 'for', 'them', 'http://cot.ag/dfjDPf', 'RT', '@grantimahara', ':', \"I'm\", 'glad', 'we', 'in', 'a', 'world', 'where', 'a', 'little', 'kid', 'can', 'wear', 'a', 'cape', 'to', 'the', 'airport', '.', '@wax_ecstatic', 'Replace', 'Mansfield', 'with', 'Azle', '&', 'I', 'can', 'totally', '!', 'You', 'smile', 'I', 'smile', '!', 'And', 'from', 'now', 'to', 'my', 'very', 'last', 'breathThis', 'day', \"I'll\", 'cherishYou', 'look', 'so', 'beautiful', 'in', 'whiteTonight', 'Great', 'touring', 'with', 'Lt', '.', '.', 'We', 'visited', 'with', 'at', 'White', 'Oaks', '...', 'http://fb.me/KkGbtk4W', 'ROTFLMAO', 'my', 'nigga', '@Qui_to_Success', 'was', 'gonna', 'in', 'on', 'that', '#TT', '#MyExGirlfriend', '.', 'Lol', \"I'm\", 'over', 'diein', 'lol', 'RT', '@TIPStrategies', ':', 'RT', '@jen_martinez', ':', 'Dept', 'of', ',', 'grants', '$500k', 'to', 'metrics', 'for', '#econdev', 'http://ow.ly/2ZYzd', 'RT', '@Solodagod', ':', 'Miami', 'put', 'a', 'fork', 'in', 'it', '...', '@theunabeefer', 'Man', ',', 'would', 'you', 'look', 'at', 'that', 'scenery', '.', 'A', 'man', 'could', 'die', 'happy', 'with', 'a', 'like', 'that', '.', '#potsandpans', 'My', 'pen', 'died', '....', \"it's\", 'the', 'only', 'pen', \"i've\", 'ever', 'used', 'in', '.', \"I'm\", 'lost', 'with', 'out', 'you', '.', '#win', 'Witch', 'by', 'Kalayna', 'Price', 'http://fang-tasticbooks.blogspot.com/2010/10/guest-blog-and-giveaway-with-kalayna.html', '@jsun187', 'lol', '@', '.', 'Congrats', 'family', '!', 'RT', '@chimaincalgary', \"Ford's\", 'on', '@CBCAsItHappens', 'http://bit.ly/cq3ZZk', '#voteto', '#yyccc', '|', 'Interesting', 'juxtaposed', 'to', \"Shaheen's\", 'speech', 'It', 'is', 'a', 'good', 'day', 'to', 'get', 'inspired', 'on', '@TODAYSHOW', '-', '@edward_burns', ',', '@ApoloOhno', 'and', '@cakelovewarren', '.', \"I'm\", 'totally', 'the', 'movie', '&', 'books', '!', 'Main', 'Girl', '>', 'Phone', 'full', 'of', 'bitches', '.', 'RT', '@AMorningOttawa', ':', 'UNITED', 'UPDATE', ':', 'So', 'far', 'you', 'have', 'helped', 'raise', '15.7', 'M', 'dollars', '.', 'The', 'is', '33.1', 'M', 'and', 'it', 'wraps', 'up', 'Dec', '2nd', '.', '@shaunmenary', 'Shaun', '!', '@jamieregier', 'I', 'always', 'do', '.', 'kings', 'is', 'unwalkable', '!', '(', '@djcamilo', 'voice', ')', 'for', '@mousebuddens', '#supportrealhiphop', '!', '@diego_goLden', 'lol', '?', 'RT', '@FruitPunchYoAss', '@DoinMeDailey', 'u', 'down', 'lol', '<<<', 'lol', 'doin', 'it', 'already', 'Ray', 'ray', 'all', 'day', 'out', '@jayclipp', '.', 'It', 'is', 'so', 'perfect', '.', 'Vintage', 'with', 'a', 'cause', '.', ':)', '@NumniimzZ', 'morning', '<3', '@HighImpactDsgns', 'onyx', '???', 'onyd', '=', 'oh', 'no', 'you', \"didn't\", '.', 'i', 'think', 'you', 'made', 'that', 'one', 'up', '@sotinafunk', 'no', 'one', 'cares', '.', 'are', 'used', 'to', 'foolish', 'for', 'attention', '.', \"they'll\", 'just', '*', 'shrug', '*', \"'\", 'Rent', 'Too', 'Damn', \"'\", 'NY', 'Jimmy', 'McMillan', 'inspires', 'talking', '-', 'The…', 'http://goo.gl/fb/1Y22x', '#palin', '#teaparty', '@SBGVoltage', 'Just', 'to', 'clarify', 'when', 'you', 'cleared', 'your', 'browser', 'cookies', 'you', 'also', 'cleared', 'the', 'browser', 'cache', '?', '^SM', 'RT', '@FrankAdman', ':', 'Name', 'says', 'it', 'all', ':', 'http://badnewsrobot.com', 'Not', 'a', 'scam', '.', \"It's\", 'for', 'real', '.', 'Might', 'be', 'the', 'only', 'Robot', 'you', 'can', 'trust', 'on', 'the', '...', 'I', 'pick', 'my', 'nose', 'wit', 'my', 'penis', 'Well', 'my', 'night', 'away', 'from', 'here', \"didn't\", 'last', 'too', 'long', '.', 'I', 'want', 'the', 'new', '3', 'coming', 'out', 'for', 'Xbox', '360', '-', 'so', 'who', 'wants', 'to', 'buy', 'it', 'for', 'me', '?!', 'LoL', 'why', 'did', 'they', 'add', 'league', 'to', '???', 'I', 'miss', 'Jenkins', 'being', 'with', 'us', 'everyday', ';(', 'RT', '@AAACarolinas', ':', 'Looking', '4', 'a', 'new', '?', '(', '@studiobanks', ',', 'maybe', '?', ')', 'AAA', 'Auto', 'Sales', 'makes', 'it', 'easy', 'http://trp.la/byLOZD', '28', 'Inspirational', 'Examples', 'of', 'Well', 'Designed', 'Contact', 'http://bit.ly/9ThfCn', 'Lol', 'we', 'dumb', '...', 'But', 'i', 'like', 'that', 'xnevershoutbrianna', 'asked', ':', 'I’M', 'SORRY', '!', 'ITS', 'TEN', 'HER', 'AND', 'I', 'HAS', 'SCHOOL', 'TOMORROW', 'AND', 'I', 'GOTTA', 'A', 'SHOWER', '.', 'WAIT', 'WAIT', '...', 'http://tumblr.com/xjcncmwl1', '#PrayForIndonesia', ',', 'RT', '@visualsurgery', ':', '@dheanaya', '@Ardhieeto', '@BinalleFabolous', '@hany_oiz', '@RicadRici', '@ata86', '(', 'cont', ')', 'http://tl.gd/6m3nns', 'RT', '@iFuckDickNPussy', ':', '#SayNo2', 'flat', 'do', 'i', 'have', 'to', 'be', 'a', 'slut', 'to', 'get', 'attention', 'wtf', ':', 'http://yearbook.com/a/pgn12', '@NicoleDaboub', \"that's\", 'pretty', 'hawt', 'is', 'that', 'all', 'of', 'the', 'outfit', '?', 'Exactly', 'how', 'I', 'expected', 'it', 'I', 'wish', 'I', 'can', 'rewind', 'time', 'n', 'go', 'way', 'back', 'to', 'when', 'I', 'was', 'playing', 'with', 'my', 'n', 'set', 'with', 'my', 'imaginary', 'it', 'was', 'much', 'better', 'then', '.', 'Photo', ':', 'AWWWWW', '.', 'Honestly', 'Imma', 'Lakers', 'just', 'cuz', 'they', 'are', ':P', 'Joking', '.', 'I', 'don’t', 'even', 'have', 'a', 'favorite', ':/', 'http://tumblr.com/xssngkmfi', '@Bash_TI', '<33', 'Love', 'Yur', '@DjAnArchy', 'LMAOOOO', 'I', 'gotta', 'that', 'does', 'that', '?!?', 'Who', '??', 'Help', 'us', '500', '!', 'RT', '\"', '#GlareX2Mender', 'products', '!', 'All', 'this', 'in', 'one', '!', 'Details', 'here', 'http://bit.ly/d6UmZd', '.', 'RT/Follow', '@Vivitone', 'to', 'win', '!\"', '@cutecanukgirl', '@NelizMD', 'Just', \"don't\", 'Nel', 'near', 'him', '...', \"she's\", 'all', 'the', 'fine', 'art', 'of', 'decapation', 'lately', '*', 'cookoo', 'cookoo', '*', 'RT', '@soulcooljay', ':', 'Coultrain', '–', 'Green', '-', 'almost', 'forgot', 'how', 'brilliant', 'this', 'was', '...', '@seymourliberty', '♫', 'http://blip.fm/~xxe88', 'This', 'lil', 'girl', 'got', 'some', 'unnecessary', 'tattoos', 'but', 'the', 'art', 'work', 'is', 'superb', '.', 'RT', '@MacCoverGirl', ':', 'RT', '@gracefuldelta', ':', 'allows', 'us', 'to', 'experience', 'the', 'low', 'points', 'of', 'life', 'in', 'to', 'teach', 'us', '...', 'http://tmi.me/2DoEF', '@harvepierre', 'did', 'you', 'receive', 'the', 'i', 'sent', 'earlier', 'today', '???', 'Let', 'me', 'know', 'por', 'favor', ':)', '@RedB3rryCARTER', 'but', 'you', 'gotta', 'think', 'Nigga', 'they', 'teams', 'that', 'already', 'got', 'the', 'chemistry', 'not', 'lookn', '4', 'it', 'Find', 'great', 'domains', 'for', '!', 'Come', 'and', 'check', 'our', 'domains', 'for', 'section', '.', 'List', 'your', 'domains', 'for', '$', '/year', 'http://sns.ly/5Jx53', 'Carving', 'a', 'Pumpkin', 'With', 'a', 'Gun', '[', ']:', 'This', 'man', 'is', '.', 'His', 'pump', '...', 'http://bit.ly/dbTukl', '@agingjoy', 'Excuse', 'me', '?', 'Uppity', '?', '::', 'confused', '::', 'GLC', '-', 'The', 'Light', 'http://ping.fm/OJQ71', '@Bieberlove96', 'oh', 'I', 'sooo', 'am', '!!', 'Lol', 'waiting', 'to', 'see', 'if', \"he's\", 'gonna', 'me', 'back', '!', 'And', 'it', \"doesn't\", 'seem', 'like', 'it', '.', '#howcome', 'there', 'is', 'so', 'many', 'bops', 'at', 'TSU', 'RT', '@StopBeck', ':', '(', 'Yup', ',', 'here', 'comes', 'the', 'guilt)', '.', 'Remember', ',', 'your', 'tweets', '*', 'do', '*', '.', 'See', 'the', 'result', 'of', '*', 'your', '*', 'efforts', 'here', ':', 'http://bit.ly/cM3pq3', 'RT', '@Real_ESPNLeBrun', ':', 'RT', '@RealKyper', ':', 'news', '.', 'Early', 'are', 'that', '#Leafs', 'may', 'surgery', 'on', 'his', 'hand', 'and', 'could', '...', '@lawmomma77', 'Say', 'what', '?', '@JOHNSAVAAGE', 'oh', 'shit', 'its', 'beyonce', 'u', 'up', 'from', 'da', 'train', 'station', 'RT', '@MISTAKT', ':', 'RT', '@GQstatus', ':', 'RT', '@DJ_Ranga', '&', '@GQstatus', ',', '@MistaKT', '&', '@DBG936', 'present', '\"', 'Get', 'Or', 'Go', 'Broke', '\"', 'on', '@ThatCrack', '...', 'http://tmi.me/', '...', 'Just', 'watched', 'Iron', '2', ',', 'what', 'a', 'nearly-perfect', 'superhero', 'movie', '!', 'So', 'good', 'you', \"don't\", 'even', 'mind', 'Rourke', '!', '(', 'J/k', ',', 'he', 'was', 'great', 'too', ')', 'Agree', 'that', 'humanizing', 'brand', 'is', 'paramount', 'but', 'associating', 'w', '1', 'personality', 'has', 'big', 'risk', ':', 'Dell', ',', ',', 'Tiger', ',', ',', '...', '#brandchat', '@LilHimQueenB', '@CuteKidd23', '@RighteousBoi', '@RipJasmyne425', '@dabossmane', '@duric2smooth', 'HATIN', \"that's\", 'crazy', ',', 'you', 'unprepared', '.', \"It's\", 'already', 'to', 'be', 'one', 'of', 'Those', 'Days', '.', 'I', 'got', 'woken', 'up', 'by', 'my', 'tv', 'falling', 'on', 'tha', 'floor', '.', 'Nice', ',', 'right', '?', '-_-', '@AndyMilonakis', 'congrats', 'on', 'the', '100k', 'Rogue', '.', 'No', '.', 'I', \"didn't\", 'do', 'it', 'yet', '.', 'ghdjfsshdfjg', '.', 'Should', 'I', '.', ',', 'right', 'now', '?', 'Larry', 'To', 'Prove', 'HP’s', 'New', 'CEO', 'Stole', 'His', 'Software', '-', 'http://tinyurl.com/272edn8', 'its', 'really', 'weird', 'how', 'FB', 'shows', '\"', 'photo', 'memories', '\"', 'of', 'the', 'hottest', 'girl', 'to', 'me', 'on', 'the', 'side', '@BJNemeth', 'was', 'asking', 'walk', 'on', 'RT', '@FunnyOrFact', ':', 'Cheating', 'on', 'the', 'person', 'u', 'LOVE', '.', '#WhoDoesThat', 'R', 'E', 'T', 'W', 'E', 'E', 'T', 'if', 'u', 'DONT', 'cheat', '&', \"u're\", 'loyal', '&', 'faithful', '.', '-', '@iRespectFemales', 'I', 'can', 'usually', 'handle', 'the', '.', 'The', 'day', 'of', 'the', 'Dallas', 'show', 'I', 'was', 'ok', 'but', 'today', 'I', \"can't\", 'seem', 'to', 'get', '.', 'Jones', 'Get', '3D', 'Treatment', '&', 'Re-Release', ':', 'Following', 'up', 'on', 'George', 'Lucas', \"'\", 'decision', 'to', 're-release', 'the', 'Star', '...', 'http://bit.ly/99ZyaY', 'Denise', 'Schump', 'chosen', 'for', 'Teacher', 'Tuesday', 'honor', ':', '...', 'with', 'a', 'and', 'a', 'collection', 'of', 'gifts', '.', 'Shump', 'said', 'she', 'w', '...', 'http://bit.ly/as5RPc', 'RT', '@ThatNicholas', ':', '10:55', 'ಠ◡ಠ', '@lewismd13', 'true', '.', 'Then', 'again', 'you', 'post', 'several', 'times', 'a', 'week', 'and', 'I', 'only', 'post', 'once', '.', 'So', 'have', 'to', 'get', 'it', 'all', 'in', 'haha', '.', 'This', 'next', 'one', 'is', 'shortish', 'My', 'head', 'is', 'killing', 'me', 'good', 'night', 'to', 'all#', 'fuck', 'the', 'rest', 'of', 'yall', 'lol', 'I', 'kno', 'thats', 'husband', ':)', 'ily', 'win', 'or', 'lose', 'RT', '@Sir_Freshiest_J', 'RT', '@KingJames', \"wasn't\", 'built', 'in', 'a', 'Day', '!', 'Work', 'http://tl.gd/6m0vlp', '@robertokaercher', 'Yes', '.', 'and', 'how', 'your', 'school', '?', '@ZCOOP', 'I', 'love', 'it', 'when', 'you', 'talk', 'contractor', '.', 'It', 'keeps', 'me', 'at', 'night', '.', 'RT', '@MichaelKors', ':', 'Check', 'me', 'out', 'as', 'a', 'devil', ',', 'age', '5', '.', 'Email', 'ur', 'halloween', 'to', 'events@michaelkors.com', ',', 'win', 'the', 'of', 'the', 'season', '!', 'http://ow', '...', 'RT', '@FactsAboutBoys', ':', 'out', 'of', 'all', 'your', 'lies', ',', '\"', 'i', 'love', 'you', '\"', 'was', 'my', 'favorite', '.', '#factsaboutboys', 'New', 'tweetaway', '!', 'Follow', '@Dermstore', '&', 'RT', 'to', 'enter', 'to', 'win', 'a', 'prize', 'from', 'Obagi', '!', '1', 'winner', 'every', 'day', 'this', 'week', '!', 'http://budurl.com/DSobagi', 'Good', '!', 'Lmao', 'everybody', 'tweeted', 'bout', 'just', 'now', '@Shells7474', 'same', 'year', '?', 'RT', '@KiLLa_iNk', ':', \"Grown'n\", 'up', 'as', 'a', 'kid', '...', 'I', 'thought', 'I', 'could', 'get', 'any', 'girl', '...', 'who', 'was', 'I', \"fool'n?!\", 'The', 'top', 'priority', 'when', 'moving', 'to', 'a', 'new', '?', 'Finding', 'a', 'dresser', ',', 'of', '.', 'Under', 'the', 'dryer', 'now', '.', \"We'll\", 'see', 'if', 'this', 'place', 'is', 'a', 'keeper', '!', 'to', 'Drop', 'OS', '10.4', 'Tiger', 'Support', '?', 'Say', 'It', 'Isn’t', 'So', '-', 'http://ht.ly/2ZsDA', '#education', '#teched', '#elearning', '#RayRay', '#ThatIsAll', '!.', 'man', 'real', 'i', 'fuck', 'wit', 'a', 'nikka', 'from', 'the', 'bottoms', 'imgood', '.', 'com', 'http://dld.bz/y5fb', '@MClark_52', 'just', 'down', 'calmly', 'with', 'her', '&', 'tell', 'her', 'every', 'fucking', 'that', 'is', 'annoying', 'you', '.', 'See', 'if', 'she', 'stays', '.', 'RT', '@imacsweb', ':', 'Dealers', ':', 'Before', 'you', 'get', 'sold', '\"', 'an', 'for', 'that', '\",', 'out', 'what', 'smartphone', 'users', 'actually', 'do', 'http://bit.ly/9Hyfcr', '(', 'via', '@eMa', '...', '@NickLikeWoe', 'YES', '.', '\"', 'Actin', \"'\", 'like', 'a', ',', 'finna', 'get', 'you', 'hurt', '\"', '-', 'Rihanna', '@MY_CHERRY_WET', 'head', 'They', 'to', 'do', 'an', 'episode', 'of', 'Cops', 'here', 'in', 'Cleveland', 'I', 'missed', 'house', 'of', 'glam', ':', '0', '(', 'in', 'the', 'room', 'before', '1', 'yess', '!', '!', 'time', 'til', 'practicee', 'No', 'injuries', 'in', 'three', 'shooting', 'incidents', 'in', 'Muskegon', ':', 'Chronicle/Jeffrey', 'BallThe', 'owner', 'of', 'a', 'home', 'i', '...', 'http://bit.ly/bb1m4K', '@digg_dugg', 'since', 'I', \"didn't\", 'know', 'what', 'the', 'you', 'were', 'in', 'the', 'first', 'place', ',', 'I', 'overlooked', 'said', 'spelling', 'mistakes', '...', '@InMyCrazyMind', 'i', 'have', 'to', 'turn', 'in', 'for', 'else', '-_-', 'RT', '@CapoRo722', ':', 'Lincecum', 'lookin', 'real', 'anxious', 'heyyyy', 'Hiiiiiiii', 'plzzzzzzzzz', 'plzzzzzzz', '+22', 'RT', '@gadling', ':', 'Five', 'tips', 'to', 'reduce', 'your', 'health', 'risk', 'while', 'eating', 'food', '|', 'Gadling', '|', 'http://aol.it/d9zm2R', \"Couldn't\", 'be', 'more', 'happy', 'for', '&', '!!', 'Just', 'saw', 'a', 'for', 'the', 'new', 'goldeneye', 'AHHHHH', '!!!!', 'If', 'i', 'die', 'dont', 'cry', 'just', 'get', 'and', 'fly', 'with', 'me', '..', 'Naw', 'we', 'pressd', 'da', 'nigga', 'he', 'did', 'some', 'slimey', 'shit', 'it', 'was', '2', 'of', 'us', 'four', 'of', 'dem', 'and', 'dey', 'was', 'some', 'brolik', 'grown', 'ass', '@Dope_Montana', '@DJJUSTN', 'yes', 'real', 'soon', '!!!', 'cant', 'wait', '!', '=)', '@bowlerhatlover', '@someonesmissing', 'thank', 'u', '!', 'Watch', 'it', 'when', 'I', 'get', 'home', '!', ':p', 'Monaco', 'asking', 'customers', 'to', 'pics', 'for', 'its', 'new', 'blog', '&', 'they', 'supply', 'the', 'http://bit.ly/aXvNCf', 'Sugary', 'May', 'Raise', 'Diabetes', 'Risk', ':', 'http://bit.ly/cTwLpF', 'via', '@addthis', '#TDFAMILY', 'YOU', 'MUST', 'ME', 'IF', 'YOU', 'NOT', 'BE', 'AT', 'EITHER', 'MEETING', '#Arcticmonkeys', '-ibetyoulookgoodonthedancefloor', '@Chikolarev', 'lol', 'ohhhhh', 'tru', 'tru', 'hahaha', 'gotta', 'stay', 'safe', '!!!!', 'I', 'just', 'put', 'extra', 'granola', 'in', 'my', 'cereal', '.', 'You', 'may', 'feel', 'as', 'if', 'you', 'have', 'out', 'of', 'time', 'because', 'there', 'are', '...', 'More', 'for', 'Gemini', 'http://twittascope.com/?sign=3', 'kinda', 'pissed', 'but', 'still', 'got', 'til', 'friday', '@JustyBiebsx3', 'Why', 'u', 'put', 'my', 'in', 'ur', 'tweets', '?', 'HAHAHA', '#TeamLakers', '#TeamKobe', '#HandsDown', '#Nuffsaid', '@royalprettygyal', 'good', 'gorgeous', '@FireMedic_Eric', 'Nooooooo', ':(', '!', \"don't\", 'say', 'thaaaaat', '.', 'Dis', 'nigga', 'just', 'went', 'on', 'First', '48', '!', 'UNC', 'lookin', \"'\", 'good', 'right', 'now', '.', 'Might', 'start', 'this', '@MoeKhan19', 'Ha', '.', 'Get', 'one', 'of', 'them', '!', '@kiddotrue', 'fashionista.com', 'covered', 'the', 'story', '@kusshmeboricua', 'rt', 'http://twitpic.com/2yt9l6', 'HalloweenNight', '!', 'Photo', ':', 'http://tumblr.com/xu0nbusxi', 'RT', '@J_Bieber_Facts', ':', 'Tweet', 'me', 'if', \"you're\", 'online', 'and', 'bored', '!', ':)', 'Homeboy', 'Sandman', ':', '\"', 'Calm', 'Tornado', '\"', '[', ']', 'http://bit.ly/9b85o3', 'RT', '@iPennyAuctions', ':', '@Beezid', '-', 'BEEZID', 'is', 'celebrating', 'their', '1', 'year', 'Birthday', 'and', 'giving', 'YOU', 'the', 'greatest', 'gift', 'of', 'all', '!', 'out', 'Beezid™', 'Bu', '...', 'RT', '@akpierce', 'I', 'am', 'having', 'the', 'most', 'awesome/inappropriate', 'conversation', 'with', '@pnuts_mama', '@rachelmarianne', '@amylou890', 'right', 'now', '.', '#guesswhatabout', 'to', 'everyone', 'who', 'likes', 'us', 'and', 'to', 'Erica', 'Haspel', 'for', 'being', 'the', '1,000', 'th', 'to', '\"', 'Like', '\"', 'our', 'page', '!', 'LMFAOOOOOOOOOOOOOOO', 'TANAYA', 'BRODY', 'LOOK', 'LIKE', 'A', 'RT', '@MayslesCinema', ':', '\"', 'Garcia’s', 'Playground', 'Basketball', 'Film', 'Festival…', '100%', 'APPROVED', '\"', 'http://bit.ly/cYKBtY', 'Early', 'TRIF', ':', 'Specific', '#2', 'no', 'longer', 'a', 'problem', ',', 'I', 'guess', '.', 'New', '#2', 'expected', 'this', 'weekend', '.', 'Agreed', '!!!', 'RT', '@RioOfLaw25', ':', 'It', \"doesn't\", 'to', 'much', 'to', 'turn', 'me', 'on', 'BUT', 'it', 'really', \"don't\", 'much', 'to', 'turn', 'me', 'OFF', '!', \"BJ's\", 'is', 'fuckin', 'gross', 'and', 'a', 'waste', 'of', 'RT', '@TBManOfTheYear', \"I'm\", 'gettin', 'a', 'lil', 'sad', '...', '<<', 'y', '??', 'Ur', 'not', 'coming', '??', '\"', \"I'm\", 'sure', 'Primo', 'is', 'very', 'motivational', '.\"', '@YeseniaJasmin', '=)', '@KOTCBLADE', 'my', 'bad', 'i', 'said', 'yes', 'sir', 'i', 'text', 'bac', 'on', 'my', 'go', 'thru', 'i', 'guess', 'Catching', 'up', 'with', 'Bela', '!', '(@', '48', '1221', '6th', 'Ave', ')', 'http://whrrl.com/e/ibUyw', 'Vacation', 'In', 'My', 'Mind', 'the', 'movie', 'drops', 'March', '17th', '.', 'Wrote', 'the', 'in', '09', \"'\", 'way', 'before', 'emo', 'man', 'made', 'his', 'movie', '.', '#FrosB4Hoes', '-Biggity', '@Boddingtons', 'Bone', 'taken', 'out', ',', 'tendon', '.', \"i'm\", 'too', 'lazy', 'to', 'get', '&', 'get', 'dressed', '.', 'just', 'too', 'lazy', ',', 'i', 'hope', 'you', 'understand', '.', 'Gotta', 'love', 'getting', 'pulled', 'over', 'right', 'outside', 'your', 'office', '...', '#goodstarttotheday', ':-\\\\', '@MrBluz', 'lets', 'Support', 'Breast', 'Cancer', 'click', 'on', 'the', 'link', '$5', 'from', 'each', 'sale', 'donated', 'http://ht.ly/30xrt?=njk2', 'My', 'cousin', 'is', 'deff', 'losing', 'ha', 'fckn', 'mind', '!', 'smh', 'Tried', 'to', 'read', 'more', 'of', 'Amy', \"McKay's\", \"'\", 'The', \"',\", 'but', 'it', 'just', \"isn't\", 'the', 'same', 'without', '@Drehal', 'reading', 'it', 'aloud', '.', '@sambolicious69', 'yess', 'i', 'am', '@MiGLBeatz', 'http://twitpic.com/31d68i', 'New', 'blog', 'post', ':', 'Camping', 'creates', 'strong', 'family', 'bonds', 'http://bit.ly/cC4oRK', '@RyanGerren', 'close', 'enough', 'to', 'smell', 'ittt', 'RT', '@vjdaniel', ':', '#prayforindonesia', 'http://myloc.me/dvLRd', 'Halloween', 'Activities', 'http://ping.fm/QIhmX', 'RT', '@HeyKikO', 'Every', 'girl', 'lives', 'for', 'the', '\"', 'unexpected', 'hugs', 'from', 'behind', '\"', '<', 'I', \"wouldn't\", 'say', '\"', '\"...', 'but', 'they', 'r', 'nice', 'How', 'to', 'Create', 'the', 'Perfect', 'Mudroom', '(', '10', 'photos', '):', 'A', 'mudroom', 'is', 'a', 'terrific', 'room', 'to', 'have', 'in', 'your', 'a', '...', 'http://bit.ly/bZiTd2', '#Home', 'Indonesians', 'try', 'to', 'return', 'to', 'homes', 'on', 'Mount', 'Merapi', '–', 'BBC', ':', 'The', 'try', 'to', 'return', 't', '...', 'http://bit.ly/9RqIbO', 'JK', '@clinteraction', 'looking', 'for', 'the', 'same', 'Life', 'is', 'like', 'photography', ',', 'we', 'use', 'the', 'negatives', 'to', '.', 'RT', '@petershankman', ':', 'Surrounded', 'by', 'in', 'uncomfortable-looking', 'ties', 'at', 'ORD', '.', \"I'm\", 'in', 'a', 'and', 'jeans', ',', 'various', 'entities', 'that', 'I', '...', '@wood_thrush', \"I'm\", 'just', 'a', 'normal', 'girl', ':', '3', 'Great', 'collection', 'of', '...', 'http://bit.ly/9mTZk0', 'RT', '@GlobalFundWomen', 'Our', 'time', 'is', 'now', '!', 'Support', 'women', 'as', 'full', 'peacemaking', 'partners', '.', '#Make1325real', '&', 'sign', 'the', 'petition', ':', 'http://bit.ly/b0Rakg', '@lilmetch', 'Oh', '!', 'Today', 'is', 'day', '!', 'Go', '@Rangers', '@ishakeitup', 'I', 'was', 'really', 'hoping', \"y'all\", 'would', 'be', 'driving', 'through', 'Albuquerque', 'on', 'Halloween', '...', 'I', 'turn', '21', 'and', \"you're\", 'the', 'authority', 'on', 'cocktails', '.', '@MelanieDenmark', 'im', 'Tryin', 'to', 'out', 'the', 'But', 'is', 'the', 'ONLY', 'one', 'with', 'the', 'RT', '@martinquest', 'Wikipedia', 'is', 'great', 'idk', 'what', 'anybody', 'says', '.', 'anyone', 'that', 'disagrees', 'is', 'a', 'fucking', '#mark', '#trickassbuster', '★ⓕⓤⓒⓚⓘⓝⓕⓞⓛⓛⓞⓦⓝⓞⓦ★', '@iEATiT_n_BEATiT', '@iT0NGUEuSQUIRT', '@AmayaLei', '@iiNenaBoo', '@BarbaraTheDoll', '@LookMa_AllGolds', '@SexyazzCC', '@IFukdHerMouth', '@NZAfro', 'And', 'yall', 'knew', 'heat', \"wasn't\", 'fuckin', 'wit', 'the', '@jazzyjaztho', 'hahaha', 'lmao', '@', 'mutha', 'bustin', '..', 'and', 'why', 'not', 'that', 'seems', 'like', 'a', 'fun', 'game', 'pahaha', 'jkaay', 'jkaay', 'and', 'Advanced', 'Technology', '/', 'Jaeger', 'and', ':', 'Shenzhen', 'based', 'and', '...', 'http://bit.ly/avRpiC', 'If', 'you', 'reading', 'this', 'Marshall', 'blog', ',', 'you', \"can't\", 'really', 'understand', 'media', ',', 'sorry', '.', 'http://marshallandme.com/', 'RT', '♥', 'it', 'lol', '@Obedbrown', ':', 'Thanks', 'boo', '!', ';-)', 'Gotta', 'keep', 'the', 'GOOD', 'in', 'ALL', 'areas', '!', 'LOL', 'RT', '@CherieMCampbell', '*', 'great', 'choice', 'of', '*', 'lmao', '@MsYellaMulann', '@Ra_StayViolatin', 'call', 'us', 'now', 'lol', 'y', 'not', 'I', 'just', 'won', 'this', 'free', 'auction', ':', '3-bikes', 'an', 'a', 'caddy', 'combo', '!!', 'http://listia.com/148HW?r=36159', '@GlassManyColors', 'Thank', 'you', 'so', 'much', 'for', 'the', 'mention', '&', '!', 'Make', 'my', 'day', '!!', '@AUsPriceless', 'come', 'on', 'out', 'there', '...', \"don't\", 'b', 'scared', 'now', '..', 'A', 'Chance', 'To', 'Win', 'A', 'Pocket', 'Devil', 'HD', 'Promo', 'Code', 'With', 'A', 'Retweet', 'Or', 'Comment', 'http://t.co/yqyJ3kV', 'via', '@appadvice', '#teamlakers', \"ya'll\", 'ready', '??', ':)', 'Be', 'the', 'first', 'to', 'know', \"what's\", 'going', 'on', '!', 'out', 'the', 'November', '\"', \"What's\", 'the', 'Buzz', '\"', 'here', 'before', \"it's\", 'even', 'back', 'from', '...', 'http://fb.me/xZFqpwxC', '#glee', '!!', 'we', 'have', 'a', 'game', 'folks', '.', '@_Zaylito901', 'u', 'already', 'know', '!', '@primesuspect', 'I', 'am', 'glad', 'u', 'to', 'make', 'it', '.', '#backchannel', 'Overcast', 'and', '55', 'F', 'at', 'Presque', 'Isle', ',', 'ME', 'Winds', 'are', 'at', '9.2', 'MPH', '(', '8', 'KT)', '.', 'The', 'is', '88%', '.', 'The', 'wind', 'chill', 'is', '52', '.', 'La', '@dartmedia', \"ya'll\", 'to', 'make', 'official', \"api's\", 'like', '@sfbart', 'does', 'for', 'developers', 'to', 'tie', 'into', 'RT', '@Jmack37', ':', 'doing', 'anything', 'cuz', 'ballin', 'on', 'this', \"2k11<Haven't\", 'made', 'up', 'my', 'mind', ',', 'worth', 'the', '?', 'I', 'would', 'like', 'to', 'thank', 'v', 'from', 'meijer', 'for', 'telling', 'me', 'geocaching', 'RT', '@matt_pc', ':', '@viviannereim', 'An', 'it', 'follows', \"Bello's\", 'top', 'journalism', 'rule', ':', 'If', 'there', 'is', 'an', 'animal', ',', 'always', 'get', \"it's\", 'name', '.', 'Makes', 'the', 'story', '1,000', '...', 'TORNADO', 'issued', 'for', 'of', 'county', 'in', 'until', '04:45', 'ET', '-', 'http://s.wx4.me/KRAHT0022', '@squabtweets', 'It', 'was', 'a', 'short', ',', 'but', 'good', 'life', '.', 'How', 'many', 'of', 'us', 'can', 'say', \"we've\", 'been', 'an', 'under-water', 'in', 'our', 'life', '?!', 'A', 'winner', 'to', 'the', 'end', '!', 'still', 'up', '4', 'no', 'reason', 'tho', '@sabrina_hudgins', 'First', 'a', 'window', 'now', 'keys', '.', \"What's\", 'next', '..', '#js']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feature encoders"
      ],
      "metadata": {
        "id": "qLskz35EjiSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num characters: len()\n",
        "\n",
        "# contains special char?\n",
        "def special_char(word):\n",
        "    return 0 if word.isalnum() else 1\n",
        "\n",
        "def special_char_at(word):\n",
        "    return 1 if ('@' in word) else 0\n",
        "\n",
        "def special_char_hash(word):\n",
        "    return 1 if ('#' in word) else 0\n",
        "\n",
        "# contrains RT?\n",
        "def RT(word):\n",
        "    return 1 if ('RT' in word) else 0\n",
        "\n",
        "# contrains URL?\n",
        "def url(word):\n",
        "  return 1 if ('http' in word) else 0"
      ],
      "metadata": {
        "id": "nBQt-rx_gZsq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model with additional features"
      ],
      "metadata": {
        "id": "p7G_FIcL-Cnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will add 6 features computed on the original center word:\n",
        "- the length of the center word\n",
        "- whether it contains any special characters  \n",
        "- whether it contains the specific special character \"@\"   \n",
        "- whether it contains the specific special character \"#\"  \n",
        "- whether it contains \"RT\"\n",
        "- whether it contrains URL\n",
        "\n",
        "I will train the model with a window size of 0 and a window size of 1."
      ],
      "metadata": {
        "id": "L6stzlRDJH4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### w=0, all vocab"
      ],
      "metadata": {
        "id": "gtI0a0P-BpBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct datasets\n",
        "train_w0_allvocab_addfeat = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_all_vocab, tag2idx=le, w=0,\n",
        "                                       feature_funcs=[len, special_char, special_char_at, special_char_hash, RT, url])\n",
        "dev_w0_allvocab_addfeat = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=0,\n",
        "                                     feature_funcs=[len, special_char, special_char_at, special_char_hash, RT, url])\n",
        "devtest_w0_allvocab_addfeat = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=0,\n",
        "                                         feature_funcs=[len, special_char, special_char_at, special_char_hash, RT, url])"
      ],
      "metadata": {
        "id": "_m8rEz1f-B1O"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=0, all vocab (random init)\n",
        "tagger_w0_addfeat = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50, nfeatures=6,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0_addfeat.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                                          model=tagger_w0_addfeat,\n",
        "                                                          name='tagger_w0_addfeat',  # file name used for using checkpoint\n",
        "                                                          optimizer=sgd,\n",
        "                                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                                          train_data=train_w0_allvocab_addfeat,\n",
        "                                                          batch_size=1,\n",
        "                                                          shuffle=True,\n",
        "                                                          val_data=dev_w0_allvocab_addfeat,\n",
        "                                                          test_data=devtest_w0_allvocab_addfeat,\n",
        "                                                          max_epochs=20,\n",
        "                                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yqYX74ljsRb",
        "outputId": "44f90c99-39fd-406b-925c-ad2aaf797130"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 31060.799917872762\n",
            "  accuracy: 0.6065382370110917\n",
            "  accuracy: 0.5978012860402406\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 17979.103977279097\n",
            "  accuracy: 0.6153531815528313\n",
            "  accuracy: 0.6038166355527899\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 13492.37095418881\n",
            "  accuracy: 0.805312317571512\n",
            "  accuracy: 0.7255756067205974\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 10987.344492305776\n",
            "  accuracy: 0.8763572679509632\n",
            "  accuracy: 0.7705870151420867\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 9082.45047461877\n",
            "  accuracy: 0.9160537069468768\n",
            "  accuracy: 0.7929890064302012\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 7938.7323304998135\n",
            "  accuracy: 0.9033858727378867\n",
            "  accuracy: 0.777224642190417\n",
            "  best model from epoch 5\n",
            "epoch 7\n",
            "  epoch loss: 7117.466688679444\n",
            "  accuracy: 0.9039112667834209\n",
            "  accuracy: 0.7749429578925534\n",
            "  best model from epoch 5\n",
            "epoch 8\n",
            "  epoch loss: 6382.096051272018\n",
            "  accuracy: 0.9169877408056042\n",
            "  accuracy: 0.7838622692387471\n",
            "  best model from epoch 5\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8031903427462815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding the 6 features, with a window size of 0, the best taggging accuracy on DEV is 79.30% from epoch 5; this best model has a tagging accuracy of 80.32% on DEVTEST.    \n",
        "Compared to the baseline tagger above, when w = 0, the additional features generated a slight improvement of tagging performance."
      ],
      "metadata": {
        "id": "-ICD2H16I4eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### w=1, all vocab"
      ],
      "metadata": {
        "id": "2oq1EQNeBv9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct datasets\n",
        "train_w1_allvocab_addfeat = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_all_vocab, tag2idx=le, w=1,\n",
        "                                       feature_funcs=[len, special_char, special_char_at, special_char_hash, RT, url])\n",
        "dev_w1_allvocab_addfeat = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=1,\n",
        "                                     feature_funcs=[len, special_char, special_char_at, special_char_hash, RT, url])\n",
        "devtest_w1_allvocab_addfeat = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=1,\n",
        "                                         feature_funcs=[len, special_char, special_char_at, special_char_hash, RT, url])"
      ],
      "metadata": {
        "id": "IulraUVeFnbd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=0, all vocab (random init)\n",
        "tagger_w1_addfeat = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50, nfeatures=6,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_addfeat.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                                          model=tagger_w1_addfeat,\n",
        "                                                          name='tagger_w1_addfeat',  # file name used for using checkpoint\n",
        "                                                          optimizer=sgd,\n",
        "                                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                                          train_data=train_w1_allvocab_addfeat,\n",
        "                                                          batch_size=1,\n",
        "                                                          shuffle=True,\n",
        "                                                          val_data=dev_w1_allvocab_addfeat,\n",
        "                                                          test_data=devtest_w1_allvocab_addfeat,\n",
        "                                                          max_epochs=20,\n",
        "                                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXhI6H0TFnY-",
        "outputId": "d6c0cf60-2345-429c-f673-713079fa16f0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 30580.26798160514\n",
            "  accuracy: 0.6035610040863981\n",
            "  accuracy: 0.6009126737191454\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 16539.012995059427\n",
            "  accuracy: 0.7466433158201985\n",
            "  accuracy: 0.7143746110765401\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 11679.177184734923\n",
            "  accuracy: 0.8239929947460596\n",
            "  accuracy: 0.7699647376063058\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 8833.067225562787\n",
            "  accuracy: 0.9167542323409223\n",
            "  accuracy: 0.8089607965152458\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 6721.740915585424\n",
            "  accuracy: 0.9287799182720373\n",
            "  accuracy: 0.800663762704833\n",
            "  best model from epoch 4\n",
            "epoch 6\n",
            "  epoch loss: 5487.215312124229\n",
            "  accuracy: 0.9408639813193228\n",
            "  accuracy: 0.8066791122173823\n",
            "  best model from epoch 4\n",
            "epoch 7\n",
            "  epoch loss: 4583.883293655882\n",
            "  accuracy: 0.961004086398132\n",
            "  accuracy: 0.8126944617299315\n",
            "  best model from epoch 7\n",
            "epoch 8\n",
            "  epoch loss: 3765.8063204532154\n",
            "  accuracy: 0.9622883829538821\n",
            "  accuracy: 0.8187098112424808\n",
            "  best model from epoch 8\n",
            "epoch 9\n",
            "  epoch loss: 3186.9526656757507\n",
            "  accuracy: 0.9661412726211325\n",
            "  accuracy: 0.8170504044803982\n",
            "  best model from epoch 8\n",
            "epoch 10\n",
            "  epoch loss: 2596.1511684314914\n",
            "  accuracy: 0.961004086398132\n",
            "  accuracy: 0.8070939639079029\n",
            "  best model from epoch 8\n",
            "epoch 11\n",
            "  epoch loss: 2214.217167658359\n",
            "  accuracy: 0.978225335668418\n",
            "  accuracy: 0.8187098112424808\n",
            "  best model from epoch 8\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8256089674498814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding the 6 features, with a window size of 1, the best taggging accuracy on DEV is 81.87% from epoch 8; this best model has a tagging accuracy of 82.56% on DEVTEST.\n",
        "Compared to the baseline tagger, when w=1, we also saw a slight improvement of tagging performance."
      ],
      "metadata": {
        "id": "DejbxLshKUAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, I experimented with not including the binary feature of whether there is any special character in the center word with w=1; thus we have 5 additional features in total this time:\n",
        "- the length of the center word\n",
        "- whether it contains the specific special character \"@\"   \n",
        "- whether it contains the specific special character \"#\"  \n",
        "- whether it contains \"RT\"\n",
        "- whether it contrains URL\n",
        "\n",
        "Results:"
      ],
      "metadata": {
        "id": "rwwoPSkgWutO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct datasets\n",
        "train_w1_allvocab_addfeat = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_all_vocab, tag2idx=le, w=1,\n",
        "                                       feature_funcs=[len, special_char_at, special_char_hash, RT, url])\n",
        "dev_w1_allvocab_addfeat = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=1,\n",
        "                                     feature_funcs=[len, special_char_at, special_char_hash, RT, url])\n",
        "devtest_w1_allvocab_addfeat = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=1,\n",
        "                                         feature_funcs=[len, special_char_at, special_char_hash, RT, url])"
      ],
      "metadata": {
        "id": "SbDOgmqmU1Pd"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=0, all vocab (random init)\n",
        "tagger_w1_addfeat = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50, nfeatures=5,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_addfeat.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                                          model=tagger_w1_addfeat,\n",
        "                                                          name='tagger_w1_addfeat',  # file name used for using checkpoint\n",
        "                                                          optimizer=sgd,\n",
        "                                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                                          train_data=train_w1_allvocab_addfeat,\n",
        "                                                          batch_size=1,\n",
        "                                                          shuffle=True,\n",
        "                                                          val_data=dev_w1_allvocab_addfeat,\n",
        "                                                          test_data=devtest_w1_allvocab_addfeat,\n",
        "                                                          max_epochs=20,\n",
        "                                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WcHXV1DU1CD",
        "outputId": "f20dd7c0-6d8c-44d6-de91-3565f077ce1b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 33888.83757842542\n",
            "  accuracy: 0.5296555750145943\n",
            "  accuracy: 0.5200165940676208\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 19600.554554729548\n",
            "  accuracy: 0.7464681844716871\n",
            "  accuracy: 0.7102260941713338\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 12578.3315212979\n",
            "  accuracy: 0.8306479859894921\n",
            "  accuracy: 0.7629122588674548\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 9132.86184076062\n",
            "  accuracy: 0.8803269118505546\n",
            "  accuracy: 0.790292470441817\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 6857.575029499546\n",
            "  accuracy: 0.9350262697022768\n",
            "  accuracy: 0.8129018875751919\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 5403.954741335297\n",
            "  accuracy: 0.9364856976065382\n",
            "  accuracy: 0.7969300974901473\n",
            "  best model from epoch 5\n",
            "epoch 7\n",
            "  epoch loss: 4395.515808695583\n",
            "  accuracy: 0.9488032691185055\n",
            "  accuracy: 0.8176726820161793\n",
            "  best model from epoch 7\n",
            "epoch 8\n",
            "  epoch loss: 3736.41588832973\n",
            "  accuracy: 0.9467600700525394\n",
            "  accuracy: 0.8039825762289982\n",
            "  best model from epoch 7\n",
            "epoch 9\n",
            "  epoch loss: 3005.7384754534537\n",
            "  accuracy: 0.9437244600116754\n",
            "  accuracy: 0.7931964322754615\n",
            "  best model from epoch 7\n",
            "epoch 10\n",
            "  epoch loss: 2589.5889414228945\n",
            "  accuracy: 0.9637478108581436\n",
            "  accuracy: 0.8081310931342045\n",
            "  best model from epoch 7\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8303513688294891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, with 5 additional features and a window size of 1, the best taggging accuracy on DEV is 81.77% from epoch 7; this best model has a tagging accuracy of 83.04% on DEVTEST. The performance on DEV is similar to that with 6 additional features, but the performance on DEVTEST is slightly better than using 6 additional features.\n",
        "\n",
        "In summary, we have seen an improvement of model performance with additional features for both window sizes 0 and 1."
      ],
      "metadata": {
        "id": "ovIgGBbDWH8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 pretrained embeddings\n",
        "\n",
        "I used the embedding for \"UUUNKKK\" when encountering words not in the pretrained embeddings, and used the embedding for \"<\\/s>\" for both \"<\\s>\" and \"<\\/s>\"."
      ],
      "metadata": {
        "id": "VN0Oh8DFSYTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fine-tuning"
      ],
      "metadata": {
        "id": "JFFQS9LsVln3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### w=0"
      ],
      "metadata": {
        "id": "Gcq7F3j0V1Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=0, fine-tuned pretrained embedding\n",
        "tagger_w0_tunedpretrained = FeedForwardNN(w=0, vocab_size=len(emb_pretrained_vocab), emb_dim=50, nfeatures=0,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0_tunedpretrained.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                          model=tagger_w0_tunedpretrained,\n",
        "                                          name='tagger_w0_tunedpretrained',  # file name used for using checkpoint\n",
        "                                          optimizer=sgd,\n",
        "                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                          train_data=train_w0_30k,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          val_data=dev_w0_30k,\n",
        "                                          test_data=devtest_w0_30k,\n",
        "                                          max_epochs=20,\n",
        "                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMTD5uzFmmSj",
        "outputId": "9086a82d-0450-4ef3-9726-9181deb567dd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 17637.035919039045\n",
            "  accuracy: 0.8551079976649153\n",
            "  accuracy: 0.8294959551960174\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 8682.902948501578\n",
            "  accuracy: 0.8724460011675423\n",
            "  accuracy: 0.8286662518149761\n",
            "  best model from epoch 1\n",
            "epoch 3\n",
            "  epoch loss: 7443.902273253319\n",
            "  accuracy: 0.8785755983654407\n",
            "  accuracy: 0.8261771416718523\n",
            "  best model from epoch 1\n",
            "epoch 4\n",
            "  epoch loss: 6872.446804946703\n",
            "  accuracy: 0.8856392294220665\n",
            "  accuracy: 0.8290811035054968\n",
            "  best model from epoch 1\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8294891140331968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 0, the best taggging accuracy on DEV is 82.95% from epoch 1 when fine-tuning pre-trained embeddings; this best model has a tagging accuracy of 82.95% on DEVTEST. Model performance improved compared to the baseline tagger."
      ],
      "metadata": {
        "id": "pVrO3CqwaOEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### w=1"
      ],
      "metadata": {
        "id": "EskuVKnEZCN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=1, fine-tuned pretrained embedding\n",
        "tagger_w1_tunedpretrained = FeedForwardNN(w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50, nfeatures=0,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_tunedpretrained.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                          model=tagger_w1_tunedpretrained,\n",
        "                                          name='tagger_w1_tunedpretrained',  # file name used for using checkpoint\n",
        "                                          optimizer=sgd,\n",
        "                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                          train_data=train_w1_30k,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          val_data=dev_w1_30k,\n",
        "                                          test_data=devtest_w1_30k,\n",
        "                                          max_epochs=20,\n",
        "                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8g-TrCtof7B",
        "outputId": "33634190-c82e-444c-8c89-33ea0db5ed49"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 12692.350564856668\n",
            "  accuracy: 0.9016929363689433\n",
            "  accuracy: 0.8626840904376686\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 6180.496532521094\n",
            "  accuracy: 0.9288382953882078\n",
            "  accuracy: 0.8691142916407384\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 4832.6801328056945\n",
            "  accuracy: 0.9440747227086982\n",
            "  accuracy: 0.8718108276291225\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 3995.835629587277\n",
            "  accuracy: 0.9546993578517221\n",
            "  accuracy: 0.8643434971997511\n",
            "  best model from epoch 3\n",
            "epoch 5\n",
            "  epoch loss: 3339.6667113360572\n",
            "  accuracy: 0.9584354932866317\n",
            "  accuracy: 0.8680771624144369\n",
            "  best model from epoch 3\n",
            "epoch 6\n",
            "  epoch loss: 2862.923839181902\n",
            "  accuracy: 0.9572679509632224\n",
            "  accuracy: 0.8581207218419415\n",
            "  best model from epoch 3\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.881008838111662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 1, the best taggging accuracy on DEV is 87.18% from epoch 3 when fine-tuning pre-trained embeddings; this best model has a tagging accuracy of 88.10% on DEVTEST. Model performance improved by a large degree compared to the baseline tagger.\n",
        "\n",
        "Additionally, we see that our models reached a stable high performance with little to no training for both w=0 and w=1 when using pretrained word embeddings."
      ],
      "metadata": {
        "id": "Hu6Y-_pMbiJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### freeze, w=1"
      ],
      "metadata": {
        "id": "dbEPZLoOZL4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have experimented with fine-tuning pretrained embeddings in the previous section; here, we experiment with freezing pretrained embeddings with w=1:"
      ],
      "metadata": {
        "id": "qpb2SYzxcVSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=1, fixed pretrained embedding\n",
        "tagger_w1_fixedpretrained = FeedForwardNN(w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50, nfeatures=0,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=True)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_fixedpretrained.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                          model=tagger_w1_fixedpretrained,\n",
        "                                          name='tagger_w1_fixedpretrained',  # file name used for using checkpoint\n",
        "                                          optimizer=sgd,\n",
        "                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                          train_data=train_w1_30k,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          val_data=dev_w1_30k,\n",
        "                                          test_data=devtest_w1_30k,\n",
        "                                          max_epochs=20,\n",
        "                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Nic-IRZF54",
        "outputId": "61a76194-e100-417e-b581-5136c5d1436b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 9565.277853610502\n",
            "  accuracy: 0.9389959136018681\n",
            "  accuracy: 0.8651732005807924\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 3710.707942625414\n",
            "  accuracy: 0.9475773496789258\n",
            "  accuracy: 0.866210329807094\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 3267.152017366822\n",
            "  accuracy: 0.9457676590776416\n",
            "  accuracy: 0.8624766645924082\n",
            "  best model from epoch 2\n",
            "epoch 4\n",
            "  epoch loss: 3070.9350762073364\n",
            "  accuracy: 0.9568009340338587\n",
            "  accuracy: 0.8680771624144369\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 2900.1935686099264\n",
            "  accuracy: 0.9558669001751313\n",
            "  accuracy: 0.8693217174859987\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 2761.0836894430704\n",
            "  accuracy: 0.9591360186806772\n",
            "  accuracy: 0.8684920141049575\n",
            "  best model from epoch 5\n",
            "epoch 7\n",
            "  epoch loss: 2662.3412220583004\n",
            "  accuracy: 0.9587273788674839\n",
            "  accuracy: 0.8716034017838623\n",
            "  best model from epoch 7\n",
            "epoch 8\n",
            "  epoch loss: 2544.4780451544757\n",
            "  accuracy: 0.9604786923525978\n",
            "  accuracy: 0.8686994399502178\n",
            "  best model from epoch 7\n",
            "epoch 9\n",
            "  epoch loss: 2413.225081450424\n",
            "  accuracy: 0.964681844716871\n",
            "  accuracy: 0.8678697365691765\n",
            "  best model from epoch 7\n",
            "epoch 10\n",
            "  epoch loss: 2349.3169193156086\n",
            "  accuracy: 0.9654991243432575\n",
            "  accuracy: 0.8720182534743829\n",
            "  best model from epoch 10\n",
            "epoch 11\n",
            "  epoch loss: 2275.460520183103\n",
            "  accuracy: 0.9647402218330414\n",
            "  accuracy: 0.8697365691765194\n",
            "  best model from epoch 10\n",
            "epoch 12\n",
            "  epoch loss: 2165.3676792832916\n",
            "  accuracy: 0.966841798015178\n",
            "  accuracy: 0.8674548848786559\n",
            "  best model from epoch 10\n",
            "epoch 13\n",
            "  epoch loss: 2077.958917092606\n",
            "  accuracy: 0.9709865732632808\n",
            "  accuracy: 0.8672474590333955\n",
            "  best model from epoch 10\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8784220737227851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 1, the best taggging accuracy on DEV is 87.20% from epoch 10 with fixed pre-trained embeddings; this best model has a tagging accuracy of 87.84% on DEVTEST. This model performance is comparable to fune-tuning pre-trained embeddings in general. However, when pre-trained embeddings are fixed, the model takes more epochs of training to reach a stable high performance than when fine-tuning the pretrained embeddings."
      ],
      "metadata": {
        "id": "3Spz4Y55d7d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### add features"
      ],
      "metadata": {
        "id": "hgl7Js6MZTYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use w=1, fine-tuned pretrianed embeddings, and 5 additional features:\n",
        "- the length of the center word\n",
        "- whether it contains the specific special character \"@\"   \n",
        "- whether it contains the specific special character \"#\"  \n",
        "- whether it contains \"RT\"\n",
        "- whether it contrains URL"
      ],
      "metadata": {
        "id": "bdBZGEj-fzdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct datasets\n",
        "train_w1_30k_addfeat = POSDataset(dataset=twpos_train, dataset_orig=orig_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1,\n",
        "                                  feature_funcs=[len, special_char_at, special_char_hash, RT, url])\n",
        "dev_w1_30k_addfeat = POSDataset(dataset=twpos_dev, dataset_orig=orig_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1,\n",
        "                                feature_funcs=[len, special_char_at, special_char_hash, RT, url])\n",
        "devtest_w1_30k_addfeat = POSDataset(dataset=twpos_devtest, dataset_orig=orig_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1,\n",
        "                                    feature_funcs=[len, special_char_at, special_char_hash, RT, url])"
      ],
      "metadata": {
        "id": "dCaYVF3xgmnN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fPhEiJTBWYtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8afe03ff-f9a2-494e-c787-006e578dadb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 10977.770401666316\n",
            "  accuracy: 0.9254524226503211\n",
            "  accuracy: 0.8574984443061605\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 5040.5131826071765\n",
            "  accuracy: 0.9332165791009924\n",
            "  accuracy: 0.8512756689483509\n",
            "  best model from epoch 1\n",
            "epoch 3\n",
            "  epoch loss: 4190.599393943067\n",
            "  accuracy: 0.9508464681844717\n",
            "  accuracy: 0.8643434971997511\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 3579.412623844145\n",
            "  accuracy: 0.9542323409223584\n",
            "  accuracy: 0.8668326073428749\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 3131.7722008914984\n",
            "  accuracy: 0.9611792177466433\n",
            "  accuracy: 0.87015142086704\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 2862.515060129004\n",
            "  accuracy: 0.9366608289550497\n",
            "  accuracy: 0.830533084422319\n",
            "  best model from epoch 5\n",
            "epoch 7\n",
            "  epoch loss: 2395.6492170088786\n",
            "  accuracy: 0.95569176882662\n",
            "  accuracy: 0.8562538892345987\n",
            "  best model from epoch 5\n",
            "epoch 8\n",
            "  epoch loss: 2237.443852695683\n",
            "  accuracy: 0.9688266199649738\n",
            "  accuracy: 0.8589504252229828\n",
            "  best model from epoch 5\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8756197456348351\n"
          ]
        }
      ],
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=1, fine-tuned pretrained embedding, additional features\n",
        "tagger_w1_tunedpretrained_addfeat = FeedForwardNN(\n",
        "    w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50, nfeatures=5,\n",
        "    layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "    layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "    pretrained_emb=emb_pretrained, emb_freeze=False\n",
        ")\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_tunedpretrained_addfeat.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                          model=tagger_w1_tunedpretrained_addfeat,\n",
        "                                          name='tagger_w1_tunedpretrained_addfeat',  # file name used for using checkpoint\n",
        "                                          optimizer=sgd,\n",
        "                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                          train_data=train_w1_30k_addfeat,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True,\n",
        "                                          val_data=dev_w1_30k_addfeat,\n",
        "                                          test_data=devtest_w1_30k_addfeat,\n",
        "                                          max_epochs=20,\n",
        "                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding features, with w=1, the best taggging accuracy on DEV is 87.02% from epoch 5 when fine-tuning pre-trained embeddings; this best model has a tagging accuracy of 87.56% on DEVTEST. Compared to the tagger with the same setup but without additional featues, the tagging performance decreased slightly, indicating that the features we developed in 1.2 is no longer helpful with fine-tuned pre-trained embeddings and w=1."
      ],
      "metadata": {
        "id": "hYEZAJEIjECL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 architecture engineering"
      ],
      "metadata": {
        "id": "y3nxf5r7mwk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will fine-tuned pretrianed embeddings configuration without additional features."
      ],
      "metadata": {
        "id": "PgehA73gnE8V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FrNoZyH_ZaS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2c5lbgyih9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3L6j0kvAih5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_fhOAK2ZaPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gszCheNsZaMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pObIy1LMXCTu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MADjYESipSlq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXA7ipUrGJmE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-xiphYGJj4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LsEoAkEetfK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Gz1BGEetc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "19hhLki5hZJMZVBLonH_fmJtvco-Lldwg",
      "authorship_tag": "ABX9TyMU/1EGEdajWWP7eR9/+FoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}