{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaangao/neural-net-pos-tagging/blob/main/NNPOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hglkm-tRE6UK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzFGdNNbZU7Q"
      },
      "source": [
        "## load raw datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZM9luBgVEWw"
      },
      "source": [
        "### load tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "jiFB8mqaNUZl"
      },
      "outputs": [],
      "source": [
        "# func to load_dataset into a list of lists of (word, tag) tuples (each inner list is a tweet)\n",
        "\n",
        "def load_dataset(data_path):\n",
        "\n",
        "  tweets = []\n",
        "  vocab = set()\n",
        "  tags = set()\n",
        "\n",
        "  with open(data_path, 'r') as file:\n",
        "\n",
        "    tweet = []\n",
        "\n",
        "    for i, line in enumerate(file):\n",
        "\n",
        "      # if line is empty, store current tweet and start a new tweet\n",
        "      if line in ['\\n']:\n",
        "        tweets.append(tweet)\n",
        "        tweet = []\n",
        "\n",
        "      # otherwise, append new word and tag to current tweet as a tuple\n",
        "      else:\n",
        "        word, tag = line.strip('\\n').split('\\t')  # split string into word and tag\n",
        "        vocab.add(word)\n",
        "        tags.add(tag)\n",
        "        tweet.append((word, tag))\n",
        "\n",
        "  return tweets, vocab, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra3mMSlcXrXn",
        "outputId": "2ebd03af-2302-4ef8-c559-788186764a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twpos_train: 1173, vocab_train: 4420\n",
            "twpos_dev: 327, vocab_dev: 1750\n",
            "twpos_devtest: 327, vocab_devtest: 1705\n"
          ]
        }
      ],
      "source": [
        "# load datasets\n",
        "\n",
        "twpos_train, vocab_train, tags_train = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-train.tsv')\n",
        "twpos_dev, vocab_dev, tags_dev = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-dev.tsv')\n",
        "twpos_devtest, vocab_devtest, tags_devtest = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-devtest.tsv')\n",
        "\n",
        "print(f'twpos_train: {len(twpos_train)}, vocab_train: {len(vocab_train)}\\ntwpos_dev: {len(twpos_dev)}, vocab_dev: {len(vocab_dev)}\\ntwpos_devtest: {len(twpos_devtest)}, vocab_devtest: {len(vocab_devtest)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTSepzofBxR7",
        "outputId": "5e7455ac-8c99-493b-f005-650c80ac45c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5991\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "# get all_vocab in train, dev, and devtest\n",
        "all_vocab = list(vocab_train.union(vocab_dev).union(vocab_devtest))\n",
        "all_vocab += ['<s>', '</s>']   # add beginning and end of sentence markers\n",
        "print(len(all_vocab))\n",
        "\n",
        "# get all_tags in train, dev, and devtest\n",
        "all_tags = list(tags_train.union(tags_dev).union(tags_devtest))\n",
        "print(len(all_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SxzpQEfVJx5"
      },
      "source": [
        "### load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKJZeoeXuJbO",
        "outputId": "9bb91655-5573-491f-929a-354229e5911e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30001 torch.Size([30001, 50])\n"
          ]
        }
      ],
      "source": [
        "# load pretrained embeddings\n",
        "\n",
        "emb_pretrained_vocab = []\n",
        "emb_pretrained = []\n",
        "\n",
        "with open('/content/drive/MyDrive/postag/data/twitter-embeddings.txt', 'r') as file:\n",
        "\n",
        "  for i, line in enumerate(file):\n",
        "\n",
        "    line_split = line.strip().split()\n",
        "\n",
        "    emb_pretrained_vocab.append(line_split[0])\n",
        "    emb_pretrained.append(list(map(float, line_split[1:])))\n",
        "\n",
        "emb_pretrained = torch.tensor(emb_pretrained)\n",
        "print(len(emb_pretrained_vocab), emb_pretrained.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIn2BFrjwN6"
      },
      "source": [
        "## construct data class with context window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW9XMJlvl-XT"
      },
      "source": [
        "### word & tag encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PFzfaaVZ4GcT"
      },
      "outputs": [],
      "source": [
        "# func: get idx in emb matrix given a word\n",
        "def get_word2idx(vocab_list):\n",
        "  word2idx = {}\n",
        "  for i, word in enumerate(vocab_list):\n",
        "    word2idx[word] = i\n",
        "  return word2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9-pwTd2J_KTm"
      },
      "outputs": [],
      "source": [
        "# for encoding words in context windows\n",
        "word2idx_all_vocab = get_word2idx(all_vocab)\n",
        "word2idx_emb_pretrained_vocab = get_word2idx(emb_pretrained_vocab)\n",
        "# tag2idx = get_word2idx(all_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PxeRokjU-xQd",
        "outputId": "6d6e888c-d087-4630-d173-18ad776c5b43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# for encoding targets\n",
        "le = LabelEncoder()\n",
        "le.fit(all_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b2fJT5pmBjc"
      },
      "source": [
        "### data class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "6wK9RgpHFxOW"
      },
      "outputs": [],
      "source": [
        "# reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, dataset:list, word2idx:dict, tag2idx:LabelEncoder(), w:int):\n",
        "\n",
        "        \"\"\"\n",
        "        wins; center_words; tags; tags_encoded\n",
        "        \"\"\"\n",
        "\n",
        "        wins = []\n",
        "        center_words = []\n",
        "        tags = []\n",
        "\n",
        "        # encode context words in each window with idx in emb\n",
        "        for tweet in dataset:\n",
        "\n",
        "            # process every center word in each tweet\n",
        "            for i, (word, tag) in enumerate(tweet):\n",
        "\n",
        "                # center word for curr obs\n",
        "                center_words.append(word)\n",
        "\n",
        "                # target of curr obs\n",
        "                tags.append(tag)\n",
        "\n",
        "                # idx for words in context window\n",
        "                win = []\n",
        "                for i in range(i-w, i+w+1):\n",
        "                    if i < 0:   # if before fist token\n",
        "                        try: win.append(word2idx['<s>'])\n",
        "                        except: win.append(word2idx['</s>'])   # if '<s>' not in emb vocab, use emb for '</s>'\n",
        "                    elif i > len(tweet)-1:    # if after last token\n",
        "                        win.append(word2idx['</s>'])\n",
        "                    else:\n",
        "                        try: win.append(word2idx[tweet[i][0]])\n",
        "                        except: win.append(word2idx['UUUNKKK'])  # use emb for unknown words\n",
        "                wins.append(win)\n",
        "\n",
        "        # encode all target tags\n",
        "        tags_encoded = tag2idx.transform(tags)\n",
        "\n",
        "        # set attributes\n",
        "        self.wins = torch.tensor(wins)\n",
        "        self.center_words = center_words\n",
        "        self.tags_encoded = torch.tensor(tags_encoded)\n",
        "        self.tags = tags\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wins)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wins[idx], self.tags_encoded[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbTzoiLQnDxw"
      },
      "source": [
        "### instantiate encoded datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "554uLQLbFw_x"
      },
      "outputs": [],
      "source": [
        "# encode datasets\n",
        "\n",
        "\n",
        "# w = 0, all vocab encoding\n",
        "train_w0_allvocab = POSDataset(dataset=twpos_train, word2idx=word2idx_all_vocab, tag2idx=le, w=0)\n",
        "dev_w0_allvocab = POSDataset(dataset=twpos_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=0)\n",
        "devtest_w0_allvocab = POSDataset(dataset=twpos_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=0)\n",
        "\n",
        "# w = 1, all vocab encoding\n",
        "train_w1_allvocab = POSDataset(dataset=twpos_train, word2idx=word2idx_all_vocab, tag2idx=le, w=1)\n",
        "dev_w1_allvocab = POSDataset(dataset=twpos_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=1)\n",
        "devtest_w1_allvocab = POSDataset(dataset=twpos_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=1)\n",
        "\n",
        "\n",
        "# w = 0, pretrained 30k vocab encoding\n",
        "train_w0_30k = POSDataset(dataset=twpos_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0)\n",
        "dev_w0_30k = POSDataset(dataset=twpos_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0)\n",
        "devtest_w0_30k = POSDataset(dataset=twpos_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0)\n",
        "\n",
        "# w = 1, pretrained 30k vocab encoding\n",
        "train_w1_30k = POSDataset(dataset=twpos_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1)\n",
        "dev_w1_30k = POSDataset(dataset=twpos_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1)\n",
        "devtest_w1_30k = POSDataset(dataset=twpos_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLiCP5wG5Wu"
      },
      "source": [
        "## 1.1 baseline neural network tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2lqvEXdNdWI"
      },
      "source": [
        "### NN model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "m16SxaOYNNr7"
      },
      "outputs": [],
      "source": [
        "# references:\n",
        "# - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "# - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "# - https://discuss.pytorch.org/t/how-to-create-mlp-model-with-arbitrary-number-of-hidden-layers/13124/2\n",
        "# - https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
        "# - https://machinelearningmastery.com/activation-functions-in-pytorch/\n",
        "\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, w, vocab_size, emb_dim, layer_sizes:list, layer_acts:list, pretrained_emb=None, emb_freeze=False):\n",
        "\n",
        "        # call parent constructor\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "\n",
        "        # set initial embeddings\n",
        "        if pretrained_emb is not None:\n",
        "            self.emb = nn.Embedding.from_pretrained(pretrained_emb, freeze=emb_freeze)\n",
        "        else:\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "            self.emb.weight.data.uniform_(-0.01, 0.01)\n",
        "\n",
        "        # set embeddings' dimensionality\n",
        "        self.emb_dim = self.emb.weight.shape[1]\n",
        "\n",
        "        # set input layer dimensionality\n",
        "        in_size = (1 + 2 * w) * self.emb_dim\n",
        "\n",
        "        # construct layers (last layer is output layer)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, layer_size in enumerate(layer_sizes):\n",
        "            if i == 0:\n",
        "                layer = nn.Linear(in_size, layer_size)\n",
        "                layer.weight.data.uniform_(-0.01, 0.01)\n",
        "                layer.bias.data.zero_()\n",
        "                self.layers.append(layer)\n",
        "                # self.layers.append(nn.Linear(in_size, layer_size))\n",
        "            else:\n",
        "                layer = nn.Linear(layer_sizes[i-1], layer_size)\n",
        "                layer.weight.data.uniform_(-0.01, 0.01)\n",
        "                layer.bias.data.zero_()\n",
        "                self.layers.append(layer)\n",
        "                # self.layers.append(nn.Linear(layer_sizes[i-1], layer_size))\n",
        "\n",
        "        # set each layer's activation function\n",
        "        self.layer_acts = layer_acts\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # retrieve context word embeddings and concat horizontally\n",
        "        x = self.emb(x).view((x.shape[0], -1))\n",
        "\n",
        "        # forward pass\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = self.layer_acts[i](x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train and eval func"
      ],
      "metadata": {
        "id": "fe_-lvTc469g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run one epoch of training\n",
        "\n",
        "def train1epoch(model, optimizer, criterion, train_dataloader):\n",
        "\n",
        "    # turn on training mode\n",
        "    model.train()\n",
        "\n",
        "    # reset epoch_loss tracker\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # iterate through mini-batches\n",
        "    for xtrain_batch, ytrain_batch in train_dataloader:\n",
        "\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        output = model(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "yyMJ77n-3S9O"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "\n",
        "def eval(model, eval_data):\n",
        "\n",
        "    # turn on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # turn off gradient calc to reduce memory consumption\n",
        "    with torch.no_grad():\n",
        "\n",
        "        ypred = torch.argmax(model(eval_data.wins), dim=1)\n",
        "        ycorrect = torch.sum(torch.eq(ypred, eval_data.tags_encoded)).item()\n",
        "\n",
        "        ytotal = len(eval_data.tags_encoded)\n",
        "        yaccu = ycorrect / ytotal\n",
        "\n",
        "    print(f'  accuracy: {yaccu}')\n",
        "    return yaccu"
      ],
      "metadata": {
        "id": "lAseCrvg-oc8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train wrapper"
      ],
      "metadata": {
        "id": "0AXYBgW1MhLk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvAwx5wy6k2Y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLm-FQECGOjC"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5n-kF2mZGOWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap encoded datasets in dataloaders: w = 0, all vocab\n",
        "train_w0_allvocab_dataloader = DataLoader(train_w0_allvocab, batch_size=1, shuffle=True)\n",
        "# dev_w0_allvocab_dataloader = DataLoader(dev_w0_allvocab, batch_size=1, shuffle=True)\n",
        "# devtest_w0_allvocab_dataloader = DataLoader(devtest_w0_allvocab, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# instantiate model w=0, all vocab\n",
        "tagger_w0 = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0.parameters(), lr=0.02)\n",
        "\n",
        "\n",
        "# train and eval\n",
        "max_epochs = 10\n",
        "epoch_losses = []\n",
        "train_accus = []\n",
        "dev_accus = []\n",
        "best_model = tagger_w0\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    # train\n",
        "    epoch_loss = train1epoch(\n",
        "                      model=tagger_w0,\n",
        "                      optimizer=sgd,\n",
        "                      criterion=nn.CrossEntropyLoss(),\n",
        "                      train_dataloader=train_w0_allvocab_dataloader\n",
        "                 )\n",
        "    epoch_losses.append(epoch_loss)\n",
        "\n",
        "    # eval on training set\n",
        "    train_accu = eval(model=tagger_w0, eval_data=train_w0_allvocab)\n",
        "    train_accus.append(train_accu)\n",
        "\n",
        "    # eval on dev set\n",
        "    dev_accu = eval(model=tagger_w0, eval_data=dev_w0_allvocab)\n",
        "    dev_accus.append(dev_accu)\n",
        "\n",
        "    # update best model\n",
        "    if dev_accu > dev_accus[-1]: best_model = deepcopy(tagger_w0)\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "print(f'best model on devtest')\n",
        "devtest_accu = eval(model=best_model, eval_data=devtest_w0_allvocab)\n"
      ],
      "metadata": {
        "id": "kadbxDiu3S5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47d7ec9-928b-4c9b-a293-b10e30037e24"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46341.057170152664\n",
            "  accuracy: 0.1514302393461763\n",
            "  accuracy: 0.1557768097904999\n",
            "epoch 2\n",
            "  epoch loss: 35529.07729223976\n",
            "  accuracy: 0.5470519556333917\n",
            "  accuracy: 0.518772038996059\n",
            "epoch 3\n",
            "  epoch loss: 21617.898748012463\n",
            "  accuracy: 0.7796847635726795\n",
            "  accuracy: 0.7129226301597179\n",
            "epoch 4\n",
            "  epoch loss: 14638.658310052168\n",
            "  accuracy: 0.865440747227087\n",
            "  accuracy: 0.7459033395561087\n",
            "epoch 5\n",
            "  epoch loss: 10505.911607737697\n",
            "  accuracy: 0.9019264448336253\n",
            "  accuracy: 0.7581414644264676\n",
            "epoch 6\n",
            "  epoch loss: 8465.48477798322\n",
            "  accuracy: 0.9077641564506713\n",
            "  accuracy: 0.7651939431653184\n",
            "epoch 7\n",
            "  epoch loss: 7212.230562191733\n",
            "  accuracy: 0.9219497956800934\n",
            "  accuracy: 0.7741132545115121\n",
            "epoch 8\n",
            "  epoch loss: 6327.465921882904\n",
            "  accuracy: 0.9155283129013426\n",
            "  accuracy: 0.7658162207010993\n",
            "epoch 9\n",
            "  epoch loss: 5831.05332768384\n",
            "  accuracy: 0.9287215411558669\n",
            "  accuracy: 0.7776394938809376\n",
            "epoch 10\n",
            "  epoch loss: 5379.076118912271\n",
            "  accuracy: 0.924810274372446\n",
            "  accuracy: 0.7761875129641154\n",
            "  accuracy: 0.7859452468204354\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7859452468204354"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hehixweh6_o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6dMXQUWvB65"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnDGpPze59pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZR3CH6cMHxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Stv1XXvcMHif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odYZficLTaGA"
      },
      "outputs": [],
      "source": [
        "# # wrap encoded datasets in dataloaders\n",
        "\n",
        "\n",
        "# # w = 0, all vocab\n",
        "# train_w0_allvocab_dataloader = DataLoader(train_w0_allvocab, batch_size=1, shuffle=True)\n",
        "# dev_w0_allvocab_dataloader = DataLoader(dev_w0_allvocab, batch_size=1, shuffle=True)\n",
        "# devtest_w0_allvocab_dataloader = DataLoader(devtest_w0_allvocab, batch_size=1, shuffle=True)\n",
        "\n",
        "# # w = 1, all vocab\n",
        "# train_w1_allvocab_dataloader = DataLoader(train_w1_allvocab, batch_size=1, shuffle=True)\n",
        "# dev_w1_allvocab_dataloader = DataLoader(dev_w1_allvocab, batch_size=1, shuffle=True)\n",
        "# devtest_w1_allvocab_dataloader = DataLoader(devtest_w1_allvocab, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# # w = 0, pretrained 30k\n",
        "# train_w0_30k_dataloader = DataLoader(train_w0_30k, batch_size=1, shuffle=True)\n",
        "# dev_w0_30k_dataloader = DataLoader(dev_w0_30k, batch_size=1, shuffle=True)\n",
        "# devtest_w0_30k_dataloader = DataLoader(devtest_w0_30k, batch_size=1, shuffle=True)\n",
        "\n",
        "# # w = 1, pretrained 30k\n",
        "# train_w1_30k_dataloader = DataLoader(train_w1_30k, batch_size=1, shuffle=True)\n",
        "# dev_w1_30k_dataloader = DataLoader(dev_w1_30k, batch_size=1, shuffle=True)\n",
        "# devtest_w1_30k_dataloader = DataLoader(devtest_w1_30k, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPhEiJTBWYtC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pralt772rIZ1",
        "outputId": "b620ca0e-8d41-40ba-b3a7-7ffaf2d9a327"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.0425e-03, -5.8820e-03, -8.6644e-03,  ..., -7.9195e-03,\n",
              "         -3.7283e-03,  7.7972e-03],\n",
              "        [ 6.4962e-03, -6.2669e-03, -3.7780e-03,  ...,  2.8969e-03,\n",
              "          7.5352e-03, -4.0234e-03],\n",
              "        [-9.9592e-03, -4.3420e-03, -5.1301e-03,  ...,  4.6443e-04,\n",
              "         -6.4169e-03, -6.4338e-03],\n",
              "        ...,\n",
              "        [ 5.9787e-03,  1.8511e-03,  8.4176e-03,  ...,  6.5441e-03,\n",
              "         -5.4045e-03, -2.6694e-03],\n",
              "        [ 8.0650e-03, -3.3898e-03,  4.1751e-03,  ..., -3.1981e-04,\n",
              "          5.9205e-03,  3.9072e-03],\n",
              "        [-5.9091e-03, -6.2013e-03, -2.4727e-03,  ..., -3.6859e-03,\n",
              "          5.9371e-04, -7.3646e-05]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# # temb = nn.Embedding.from_pretrained(emb_pretrained, freeze=True)\n",
        "# temb = nn.Embedding(len(all_vocab), 50)\n",
        "# temb.weight.data.uniform_(-0.01, 0.01)\n",
        "# temb.weight\n",
        "# # print(temb.weight.shape)\n",
        "\n",
        "# curr_input = Xtrain_w1[0].unsqueeze(0)\n",
        "# # curr_input = Xtrain_w1[:1]\n",
        "# print(curr_input.shape)\n",
        "# concatemb = temb(curr_input).view((curr_input.shape[0], -1))\n",
        "# print(concatemb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6f38FF0tM_i",
        "outputId": "e187f0a9-603e-465e-e38d-44459e7acac7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 150])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# concatemb_flat = concatemb.view((concatemb.shape[0], -1))\n",
        "# concatemb_flat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuSIUt_dvCjk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pObIy1LMXCTu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT4woDB1wW5m",
        "outputId": "df6006c9-a5b6-4946-d8ef-8844ddd0bee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46359.56888842583\n",
            "  train accu: 0.1514302393461763\n",
            "  dev accu: 0.1557768097904999\n",
            "epoch 2\n",
            "  epoch loss: 36511.22461539088\n",
            "  train accu: 0.6244016345592528\n",
            "  dev accu: 0.6050611906243518\n",
            "epoch 3\n",
            "  epoch loss: 20647.9830187872\n",
            "  train accu: 0.7897256275539989\n",
            "  dev accu: 0.7313835303878863\n",
            "epoch 4\n",
            "  epoch loss: 13142.984029729269\n",
            "  train accu: 0.883420899007589\n",
            "  dev accu: 0.751296411532877\n",
            "epoch 5\n",
            "  epoch loss: 9927.773577833897\n",
            "  train accu: 0.9056625802685347\n",
            "  dev accu: 0.7635345364032359\n",
            "epoch 6\n",
            "  epoch loss: 8590.880470508913\n",
            "  train accu: 0.9119089316987741\n",
            "  dev accu: 0.7624974071769343\n",
            "epoch 7\n",
            "  epoch loss: 7707.652470903413\n",
            "  train accu: 0.9249270286047869\n",
            "  dev accu: 0.7712092926778676\n",
            "epoch 8\n",
            "  epoch loss: 6816.784264039488\n",
            "  train accu: 0.9195563339171045\n",
            "  dev accu: 0.7660236465463597\n",
            "epoch 9\n",
            "  epoch loss: 6216.87197396993\n",
            "  train accu: 0.9262697022767076\n",
            "  dev accu: 0.7778469197261979\n",
            "epoch 10\n",
            "  epoch loss: 5744.520718641375\n",
            "  train accu: 0.9217746643315821\n",
            "  dev accu: 0.7745281062020327\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=0\n",
        "tagger_w0 = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "# len(list(tagger_w0.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w0.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w0_allvocab_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w0(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w0.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w0(train_w0_allvocab.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w0_allvocab.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w0_allvocab.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w0.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w0(dev_w0_allvocab.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w0_allvocab.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w0_allvocab.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL_N_p7hXExc",
        "outputId": "ba79281e-1261-4346-85ce-76b951009d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "  epoch loss: 36041.978227217594\n",
            "  train accu: 0.5828371278458844\n",
            "  dev accu: 0.5718730553827007\n",
            "epoch 2\n",
            "  epoch loss: 16047.907971054286\n",
            "  train accu: 0.8486281377699941\n",
            "  dev accu: 0.7600082970338105\n",
            "epoch 3\n",
            "  epoch loss: 9350.579022063437\n",
            "  train accu: 0.9163455925277292\n",
            "  dev accu: 0.796100394109106\n",
            "epoch 4\n",
            "  epoch loss: 6425.25555942486\n",
            "  train accu: 0.9440163455925278\n",
            "  dev accu: 0.8048122796100394\n",
            "epoch 5\n",
            "  epoch loss: 4911.637813769848\n",
            "  train accu: 0.9469352014010508\n",
            "  dev accu: 0.7981746525617092\n",
            "epoch 6\n",
            "  epoch loss: 4130.616831530634\n",
            "  train accu: 0.9600700525394046\n",
            "  dev accu: 0.8025305953121759\n",
            "epoch 7\n",
            "  epoch loss: 3199.3462827699595\n",
            "  train accu: 0.9615878575598366\n",
            "  dev accu: 0.7987969300974902\n",
            "epoch 8\n",
            "  epoch loss: 2535.824724089276\n",
            "  train accu: 0.9718038528896672\n",
            "  dev accu: 0.8029454470026965\n",
            "epoch 9\n",
            "  epoch loss: 2146.7702989513787\n",
            "  train accu: 0.9804436660828955\n",
            "  dev accu: 0.7998340593237917\n",
            "epoch 10\n",
            "  epoch loss: 1670.3467638214952\n",
            "  train accu: 0.9821366024518389\n",
            "  dev accu: 0.8004563368595727\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=1\n",
        "tagger_w1 = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "# len(list(tagger_w1.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w1.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w1_allvocab_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w1(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w1.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w1(train_w1_allvocab.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w1_allvocab.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w1_allvocab.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w1.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w1(dev_w1_allvocab.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w1_allvocab.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w1_allvocab.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MADjYESipSlq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nrVRsTXpSij",
        "outputId": "54f2b426-73e8-484b-fa54-6fd4733fb672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "  epoch loss: 16369.005858714794\n",
            "  train accu: 0.8449503794512551\n",
            "  dev accu: 0.8344741754822651\n",
            "epoch 2\n",
            "  epoch loss: 9320.458782576461\n",
            "  train accu: 0.8655575014594279\n",
            "  dev accu: 0.8477494295789255\n",
            "epoch 3\n",
            "  epoch loss: 8422.924471781236\n",
            "  train accu: 0.8650321074138938\n",
            "  dev accu: 0.8471271520431446\n",
            "epoch 4\n",
            "  epoch loss: 7934.014341046579\n",
            "  train accu: 0.880502043199066\n",
            "  dev accu: 0.8574984443061605\n",
            "epoch 5\n",
            "  epoch loss: 7540.962700240924\n",
            "  train accu: 0.8785755983654407\n",
            "  dev accu: 0.8531425015556938\n",
            "epoch 6\n",
            "  epoch loss: 7192.139447728374\n",
            "  train accu: 0.8861646234676007\n",
            "  dev accu: 0.8514830947936113\n",
            "epoch 7\n",
            "  epoch loss: 6820.51915438598\n",
            "  train accu: 0.8892002335084647\n",
            "  dev accu: 0.8581207218419415\n",
            "epoch 8\n",
            "  epoch loss: 6506.891840068924\n",
            "  train accu: 0.8973730297723292\n",
            "  dev accu: 0.8657954781165733\n",
            "epoch 9\n",
            "  epoch loss: 6227.353143249253\n",
            "  train accu: 0.906713368359603\n",
            "  dev accu: 0.8684920141049575\n",
            "epoch 10\n",
            "  epoch loss: 5917.889376106414\n",
            "  train accu: 0.9095738470519557\n",
            "  dev accu: 0.8645509230450114\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=1, fixed pretrained embedding\n",
        "tagger_w1_fixedpretrained = FeedForwardNN(w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=True)\n",
        "# len(list(tagger_w1_fixedpretrained.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_fixedpretrained.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w1_fixedpretrained.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w1_30k_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w1_fixedpretrained(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w1_fixedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w1_fixedpretrained(train_w1_30k.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w1_30k.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w1_30k.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w1_fixedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w1_fixedpretrained(dev_w1_30k.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w1_30k.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w1_30k.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd3f3k4VuC2i",
        "outputId": "2a461b00-91d9-4e71-c778-98bb75f93978"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 8.0050e-03,  8.8390e-03, -7.6610e-03,  ...,  3.3940e-03,\n",
              "          4.0300e-04,  2.6620e-03],\n",
              "        [ 2.0712e-01, -3.1345e-02,  9.1379e-02,  ...,  1.5570e-01,\n",
              "         -6.7304e-02, -2.5445e-02],\n",
              "        [-3.2129e-01,  5.0717e-02,  2.1766e-01,  ..., -1.6058e-01,\n",
              "          5.7263e-02,  3.8416e-01],\n",
              "        ...,\n",
              "        [-8.6111e-01,  3.4025e-01,  1.6014e-01,  ...,  4.0112e-01,\n",
              "         -3.7185e-01, -4.0742e-01],\n",
              "        [-1.0521e-01, -2.7858e-01,  4.9961e-01,  ..., -2.5552e-01,\n",
              "         -2.2169e-01,  5.1709e-01],\n",
              "        [-2.3829e-01,  5.1810e-02,  3.3805e-01,  ...,  1.7722e-01,\n",
              "         -6.0529e-02, -1.6389e-01]])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagger_w1_fixedpretrained.emb.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6uSEDEguFNm",
        "outputId": "1de49d63-0c48-49f0-b4a5-3ecd41ac6b0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 8.0050e-03,  8.8390e-03, -7.6610e-03,  ...,  3.3940e-03,\n",
              "          4.0300e-04,  2.6620e-03],\n",
              "        [ 2.0712e-01, -3.1345e-02,  9.1379e-02,  ...,  1.5570e-01,\n",
              "         -6.7304e-02, -2.5445e-02],\n",
              "        [-3.2129e-01,  5.0717e-02,  2.1766e-01,  ..., -1.6058e-01,\n",
              "          5.7263e-02,  3.8416e-01],\n",
              "        ...,\n",
              "        [-8.6111e-01,  3.4025e-01,  1.6014e-01,  ...,  4.0112e-01,\n",
              "         -3.7185e-01, -4.0742e-01],\n",
              "        [-1.0521e-01, -2.7858e-01,  4.9961e-01,  ..., -2.5552e-01,\n",
              "         -2.2169e-01,  5.1709e-01],\n",
              "        [-2.3829e-01,  5.1810e-02,  3.3805e-01,  ...,  1.7722e-01,\n",
              "         -6.0529e-02, -1.6389e-01]])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYxdX37yuK2E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLD-5yGpuKtf",
        "outputId": "d55ddf33-36c1-4451-b741-ad7a532cf5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "  epoch loss: 14670.984842691923\n",
            "  train accu: 0.891009924109749\n",
            "  dev accu: 0.8550093341630367\n",
            "epoch 2\n",
            "  epoch loss: 6710.139597005856\n",
            "  train accu: 0.9273204903677759\n",
            "  dev accu: 0.8682845882596971\n",
            "epoch 3\n",
            "  epoch loss: 4973.363695524482\n",
            "  train accu: 0.9360186806771745\n",
            "  dev accu: 0.8624766645924082\n",
            "epoch 4\n",
            "  epoch loss: 4005.304377012074\n",
            "  train accu: 0.9389375364856976\n",
            "  dev accu: 0.8651732005807924\n",
            "epoch 5\n",
            "  epoch loss: 3292.6318096834343\n",
            "  train accu: 0.9558669001751313\n",
            "  dev accu: 0.8695291433312591\n",
            "epoch 6\n",
            "  epoch loss: 2835.4244742415253\n",
            "  train accu: 0.9660828955049621\n",
            "  dev accu: 0.8732628085459448\n",
            "epoch 7\n",
            "  epoch loss: 2353.4613265826006\n",
            "  train accu: 0.9709281961471103\n",
            "  dev accu: 0.8579132959966812\n",
            "epoch 8\n",
            "  epoch loss: 2089.4233974509316\n",
            "  train accu: 0.9732049036777584\n",
            "  dev accu: 0.8660029039618337\n",
            "epoch 9\n",
            "  epoch loss: 1726.4095906020575\n",
            "  train accu: 0.9776999416228839\n",
            "  dev accu: 0.8572910184609003\n",
            "epoch 10\n",
            "  epoch loss: 1580.652004197805\n",
            "  train accu: 0.9838295388207823\n",
            "  dev accu: 0.8574984443061605\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=1, fine-tuned pretrained embedding\n",
        "tagger_w1_tunedpretrained = FeedForwardNN(w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=False)\n",
        "# len(list(tagger_w1_tunedpretrained.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_tunedpretrained.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w1_tunedpretrained.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w1_30k_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w1_tunedpretrained(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w1_tunedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w1_tunedpretrained(train_w1_30k.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w1_30k.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w1_30k.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w1_tunedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w1_tunedpretrained(dev_w1_30k.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w1_30k.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w1_30k.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaxhM2oSwWx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f85b5b-e0de-4161-e346-d7f3c0b3fea6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 5.5893e-02, -2.1001e-01, -2.9847e-02,  ..., -1.1455e-01,\n",
              "         -1.9788e-01,  2.1989e-01],\n",
              "        [ 2.8332e-01, -2.4272e-01, -6.2081e-03,  ...,  2.9925e-01,\n",
              "         -6.3570e-02,  5.4027e-02],\n",
              "        [-4.9772e-01, -1.5044e-01, -1.3602e-02,  ..., -2.1467e-01,\n",
              "         -2.3371e-03,  4.4186e-01],\n",
              "        ...,\n",
              "        [-8.6111e-01,  3.4025e-01,  1.6014e-01,  ...,  4.0112e-01,\n",
              "         -3.7185e-01, -4.0742e-01],\n",
              "        [-1.0521e-01, -2.7858e-01,  4.9961e-01,  ..., -2.5552e-01,\n",
              "         -2.2169e-01,  5.1709e-01],\n",
              "        [-4.5604e-03,  8.8334e-02,  1.7233e-01,  ..., -4.9501e-04,\n",
              "          4.8545e-02,  1.7906e-01]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "tagger_w1_tunedpretrained.emb.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXA7ipUrGJmE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-xiphYGJj4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHUOMYOzFVTG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMhURUk_ylZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idd1olUmEECE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LsEoAkEetfK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Gz1BGEetc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gZM9luBgVEWw",
        "6SxzpQEfVJx5",
        "IW9XMJlvl-XT",
        "9b2fJT5pmBjc",
        "NbTzoiLQnDxw"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "19hhLki5hZJMZVBLonH_fmJtvco-Lldwg",
      "authorship_tag": "ABX9TyPoUZNkFlkOANq+yIhKFUlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}