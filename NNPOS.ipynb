{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaangao/neural-net-pos-tagging/blob/main/NNPOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hglkm-tRE6UK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzFGdNNbZU7Q"
      },
      "source": [
        "## load raw datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZM9luBgVEWw"
      },
      "source": [
        "### load tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jiFB8mqaNUZl"
      },
      "outputs": [],
      "source": [
        "# func to load_dataset into a list of lists of (word, tag) tuples (each inner list is a tweet)\n",
        "\n",
        "def load_dataset(data_path):\n",
        "\n",
        "  tweets = []\n",
        "  vocab = set()\n",
        "  tags = set()\n",
        "\n",
        "  with open(data_path, 'r') as file:\n",
        "\n",
        "    tweet = []\n",
        "\n",
        "    for i, line in enumerate(file):\n",
        "\n",
        "      # if line is empty, store current tweet and start a new tweet\n",
        "      if line in ['\\n']:\n",
        "        tweets.append(tweet)\n",
        "        tweet = []\n",
        "\n",
        "      # otherwise, append new word and tag to current tweet as a tuple\n",
        "      else:\n",
        "        word, tag = line.strip('\\n').split('\\t')  # split string into word and tag\n",
        "        vocab.add(word)\n",
        "        tags.add(tag)\n",
        "        tweet.append((word, tag))\n",
        "\n",
        "  return tweets, vocab, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra3mMSlcXrXn",
        "outputId": "6c2a6691-9c65-4055-9eb0-ac49a93b51fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twpos_train: 1173, vocab_train: 4420\n",
            "twpos_dev: 327, vocab_dev: 1750\n",
            "twpos_devtest: 327, vocab_devtest: 1705\n"
          ]
        }
      ],
      "source": [
        "# load datasets\n",
        "\n",
        "twpos_train, vocab_train, tags_train = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-train.tsv')\n",
        "twpos_dev, vocab_dev, tags_dev = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-dev.tsv')\n",
        "twpos_devtest, vocab_devtest, tags_devtest = load_dataset('/content/drive/MyDrive/postag/data/twpos-data/twpos-devtest.tsv')\n",
        "\n",
        "print(f'twpos_train: {len(twpos_train)}, vocab_train: {len(vocab_train)}\\ntwpos_dev: {len(twpos_dev)}, vocab_dev: {len(vocab_dev)}\\ntwpos_devtest: {len(twpos_devtest)}, vocab_devtest: {len(vocab_devtest)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTSepzofBxR7",
        "outputId": "c514faee-2fd3-4fd4-c030-c44ad4cf7e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5991\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "# get all_vocab in train, dev, and devtest\n",
        "all_vocab = list(vocab_train.union(vocab_dev).union(vocab_devtest))\n",
        "all_vocab += ['<s>', '</s>']   # add beginning and end of sentence markers\n",
        "print(len(all_vocab))\n",
        "\n",
        "# get all_tags in train, dev, and devtest\n",
        "all_tags = list(tags_train.union(tags_dev).union(tags_devtest))\n",
        "print(len(all_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SxzpQEfVJx5"
      },
      "source": [
        "### load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKJZeoeXuJbO",
        "outputId": "bc88a0b8-303e-4be6-e3a7-e8b1922d76c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30001 torch.Size([30001, 50])\n"
          ]
        }
      ],
      "source": [
        "# load pretrained embeddings\n",
        "\n",
        "emb_pretrained_vocab = []\n",
        "emb_pretrained = []\n",
        "\n",
        "with open('/content/drive/MyDrive/postag/data/twitter-embeddings.txt', 'r') as file:\n",
        "\n",
        "  for i, line in enumerate(file):\n",
        "\n",
        "    line_split = line.strip().split()\n",
        "\n",
        "    emb_pretrained_vocab.append(line_split[0])\n",
        "    emb_pretrained.append(list(map(float, line_split[1:])))\n",
        "\n",
        "emb_pretrained = torch.tensor(emb_pretrained)\n",
        "print(len(emb_pretrained_vocab), emb_pretrained.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leIn2BFrjwN6"
      },
      "source": [
        "## construct data class with context window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW9XMJlvl-XT"
      },
      "source": [
        "### word & tag encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PFzfaaVZ4GcT"
      },
      "outputs": [],
      "source": [
        "# func: get idx in emb matrix given a word\n",
        "def get_word2idx(vocab_list):\n",
        "  word2idx = {}\n",
        "  for i, word in enumerate(vocab_list):\n",
        "    word2idx[word] = i\n",
        "  return word2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9-pwTd2J_KTm"
      },
      "outputs": [],
      "source": [
        "# for encoding words in context windows\n",
        "word2idx_all_vocab = get_word2idx(all_vocab)\n",
        "word2idx_emb_pretrained_vocab = get_word2idx(emb_pretrained_vocab)\n",
        "# tag2idx = get_word2idx(all_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PxeRokjU-xQd",
        "outputId": "727c167e-f34a-4a3c-da8f-c6fb761831ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# for encoding targets\n",
        "le = LabelEncoder()\n",
        "le.fit(all_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b2fJT5pmBjc"
      },
      "source": [
        "### data class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6wK9RgpHFxOW"
      },
      "outputs": [],
      "source": [
        "# reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, dataset:list, word2idx:dict, tag2idx:LabelEncoder(), w:int):\n",
        "\n",
        "        \"\"\"\n",
        "        wins; center_words; tags; tags_encoded\n",
        "        \"\"\"\n",
        "\n",
        "        wins = []\n",
        "        center_words = []\n",
        "        tags = []\n",
        "\n",
        "        # encode context words in each window with idx in emb\n",
        "        for tweet in dataset:\n",
        "\n",
        "            # process every center word in each tweet\n",
        "            for i, (word, tag) in enumerate(tweet):\n",
        "\n",
        "                # center word for curr obs\n",
        "                center_words.append(word)\n",
        "\n",
        "                # target of curr obs\n",
        "                tags.append(tag)\n",
        "\n",
        "                # idx for words in context window\n",
        "                win = []\n",
        "                for i in range(i-w, i+w+1):\n",
        "                    if i < 0:   # if before fist token\n",
        "                        try: win.append(word2idx['<s>'])\n",
        "                        except: win.append(word2idx['</s>'])   # if '<s>' not in emb vocab, use emb for '</s>'\n",
        "                    elif i > len(tweet)-1:    # if after last token\n",
        "                        win.append(word2idx['</s>'])\n",
        "                    else:\n",
        "                        try: win.append(word2idx[tweet[i][0]])\n",
        "                        except: win.append(word2idx['UUUNKKK'])  # use emb for unknown words\n",
        "                wins.append(win)\n",
        "\n",
        "        # encode all target tags\n",
        "        tags_encoded = tag2idx.transform(tags)\n",
        "\n",
        "        # set attributes\n",
        "        self.wins = torch.tensor(wins)\n",
        "        self.center_words = np.array(center_words)\n",
        "        self.tags_encoded = torch.tensor(tags_encoded)\n",
        "        self.tags = np.array(tags)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wins)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wins[idx], self.tags_encoded[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbTzoiLQnDxw"
      },
      "source": [
        "### instantiate encoded datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "554uLQLbFw_x"
      },
      "outputs": [],
      "source": [
        "# encode datasets\n",
        "\n",
        "\n",
        "# w = 0, all vocab encoding\n",
        "train_w0_allvocab = POSDataset(dataset=twpos_train, word2idx=word2idx_all_vocab, tag2idx=le, w=0)\n",
        "dev_w0_allvocab = POSDataset(dataset=twpos_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=0)\n",
        "devtest_w0_allvocab = POSDataset(dataset=twpos_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=0)\n",
        "\n",
        "# w = 1, all vocab encoding\n",
        "train_w1_allvocab = POSDataset(dataset=twpos_train, word2idx=word2idx_all_vocab, tag2idx=le, w=1)\n",
        "dev_w1_allvocab = POSDataset(dataset=twpos_dev, word2idx=word2idx_all_vocab, tag2idx=le, w=1)\n",
        "devtest_w1_allvocab = POSDataset(dataset=twpos_devtest, word2idx=word2idx_all_vocab, tag2idx=le, w=1)\n",
        "\n",
        "\n",
        "# w = 0, pretrained 30k vocab encoding\n",
        "train_w0_30k = POSDataset(dataset=twpos_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0)\n",
        "dev_w0_30k = POSDataset(dataset=twpos_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0)\n",
        "devtest_w0_30k = POSDataset(dataset=twpos_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=0)\n",
        "\n",
        "# w = 1, pretrained 30k vocab encoding\n",
        "train_w1_30k = POSDataset(dataset=twpos_train, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1)\n",
        "dev_w1_30k = POSDataset(dataset=twpos_dev, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1)\n",
        "devtest_w1_30k = POSDataset(dataset=twpos_devtest, word2idx=word2idx_emb_pretrained_vocab, tag2idx=le, w=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLiCP5wG5Wu"
      },
      "source": [
        "## 1.1 baseline neural network tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2lqvEXdNdWI"
      },
      "source": [
        "### model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m16SxaOYNNr7"
      },
      "outputs": [],
      "source": [
        "# references:\n",
        "# - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "# - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "# - https://discuss.pytorch.org/t/how-to-create-mlp-model-with-arbitrary-number-of-hidden-layers/13124/2\n",
        "# - https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_feedforward_neuralnetwork/\n",
        "# - https://machinelearningmastery.com/activation-functions-in-pytorch/\n",
        "\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, w, vocab_size, emb_dim, layer_sizes:list, layer_acts:list, pretrained_emb=None, emb_freeze=False):\n",
        "\n",
        "        # call parent constructor\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "\n",
        "        # set initial embeddings\n",
        "        if pretrained_emb is not None:\n",
        "            self.emb = nn.Embedding.from_pretrained(pretrained_emb, freeze=emb_freeze)\n",
        "        else:   # randomly init embeddings\n",
        "            self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "            self.emb.weight.data.uniform_(-0.01, 0.01)\n",
        "\n",
        "        # set embeddings' dimensionality\n",
        "        self.emb_dim = self.emb.weight.shape[1]\n",
        "\n",
        "        # set input layer dimensionality\n",
        "        in_size = (1 + 2 * w) * self.emb_dim\n",
        "\n",
        "        # construct layers (last layer is output layer)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, layer_size in enumerate(layer_sizes):\n",
        "            if i == 0:\n",
        "                layer = nn.Linear(in_size, layer_size)\n",
        "                layer.weight.data.uniform_(-0.01, 0.01)\n",
        "                layer.bias.data.zero_()\n",
        "                self.layers.append(layer)\n",
        "                # self.layers.append(nn.Linear(in_size, layer_size))\n",
        "            else:\n",
        "                layer = nn.Linear(layer_sizes[i-1], layer_size)\n",
        "                layer.weight.data.uniform_(-0.01, 0.01)\n",
        "                layer.bias.data.zero_()\n",
        "                self.layers.append(layer)\n",
        "                # self.layers.append(nn.Linear(layer_sizes[i-1], layer_size))\n",
        "\n",
        "        # set each layer's activation function\n",
        "        self.layer_acts = layer_acts\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # retrieve context word embeddings and concat horizontally\n",
        "        x = self.emb(x).view((x.shape[0], -1))\n",
        "\n",
        "        # forward pass\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = self.layer_acts[i](x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train and eval func"
      ],
      "metadata": {
        "id": "fe_-lvTc469g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run one epoch of training\n",
        "\n",
        "def train1epoch(model, optimizer, criterion, train_dataloader):\n",
        "\n",
        "    # turn on training mode\n",
        "    model.train()\n",
        "\n",
        "    # reset epoch_loss tracker\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # iterate through mini-batches\n",
        "    for xtrain_batch, ytrain_batch in train_dataloader:\n",
        "\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        output = model(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "yyMJ77n-3S9O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval\n",
        "\n",
        "def eval(model, eval_data):\n",
        "\n",
        "    # turn on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # turn off gradient calc to reduce memory consumption\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # get model predictions on eval_data\n",
        "        ypred = torch.argmax(model(eval_data.wins), dim=1)\n",
        "\n",
        "        # count correct predictions\n",
        "        ycorrect = torch.sum(torch.eq(ypred, eval_data.tags_encoded)).item()\n",
        "\n",
        "        # total num of obs in eval_data\n",
        "        ytotal = len(eval_data.tags_encoded)\n",
        "\n",
        "        # compute accuracy\n",
        "        yaccu = ycorrect / ytotal\n",
        "\n",
        "    print(f'  accuracy: {yaccu}')\n",
        "    return yaccu"
      ],
      "metadata": {
        "id": "lAseCrvg-oc8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train & eval wrapper"
      ],
      "metadata": {
        "id": "0AXYBgW1MhLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper for train & eval\n",
        "# reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "def main_process(model, name, optimizer, criterion, train_data, batch_size, shuffle, val_data, test_data, max_epochs=10, early_stopping=3):\n",
        "\n",
        "\n",
        "    # create batched iterator for train_data\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "    # initialize vars: track metrics\n",
        "    epoch_losses = []\n",
        "    train_evals = []\n",
        "    dev_evals = []\n",
        "\n",
        "    # initialize vars: track best model\n",
        "    best_dev_eval = 0\n",
        "    best_model_epoch = -1\n",
        "\n",
        "    # train and eval\n",
        "    for epoch in range(max_epochs):\n",
        "\n",
        "        print(f'epoch {epoch+1}')\n",
        "\n",
        "        # train\n",
        "        epoch_loss = train1epoch(\n",
        "                          model=model,\n",
        "                          optimizer=optimizer,\n",
        "                          criterion=criterion,\n",
        "                          train_dataloader=train_dataloader\n",
        "                    )\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "        # eval on training set\n",
        "        train_eval = eval(model=model, eval_data=train_data)\n",
        "        train_evals.append(train_eval)\n",
        "\n",
        "        # eval on dev set\n",
        "        dev_eval = eval(model=model, eval_data=val_data)\n",
        "        dev_evals.append(dev_eval)\n",
        "\n",
        "        # update best model based on dev eval\n",
        "        if dev_eval > best_dev_eval:\n",
        "\n",
        "            # save state_dict of best model so far\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/postag/models/'+name+'_best.pth.tar')\n",
        "\n",
        "            # update which epoch best_model is from\n",
        "            best_model_epoch = epoch\n",
        "\n",
        "            # update best_dev_accu\n",
        "            best_dev_eval = dev_eval\n",
        "\n",
        "        print(f'  best model from epoch {best_model_epoch+1}')\n",
        "\n",
        "        # early stopping based on dev eval\n",
        "        if early_stopping is not None:\n",
        "            if epoch - best_model_epoch >= early_stopping:\n",
        "                print('========= EARLY STOPPING =========')\n",
        "                break\n",
        "\n",
        "\n",
        "    # load state_dict of best model (modifies input model in place)\n",
        "    print(f'load best model')\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/postag/models/'+name+'_best.pth.tar'))\n",
        "\n",
        "    # eval best model on devtest set\n",
        "    print(f'eval best model on devtest')\n",
        "    devtest_eval = eval(model=model, eval_data=test_data)\n",
        "\n",
        "\n",
        "    return epoch_losses, train_evals, dev_evals, devtest_eval\n"
      ],
      "metadata": {
        "id": "yM4nxqEAOi9X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run model: w=0, all vocab"
      ],
      "metadata": {
        "id": "NNl16tuaZ8MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=0, all vocab (random init)\n",
        "tagger_w0 = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                                          model=tagger_w0,\n",
        "                                                          name='tagger_w0',  # file name used for using checkpoint\n",
        "                                                          optimizer=sgd,\n",
        "                                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                                          train_data=train_w0_allvocab,\n",
        "                                                          batch_size=1,\n",
        "                                                          shuffle=True,\n",
        "                                                          val_data=dev_w0_allvocab,\n",
        "                                                          test_data=devtest_w0_allvocab,\n",
        "                                                          max_epochs=20,\n",
        "                                                          early_stopping=3   # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9172EhnWSnnT",
        "outputId": "2e58e3b1-0dac-43c6-b4af-e0f4ad0ec2ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46347.013986587524\n",
            "  accuracy: 0.13736135434909516\n",
            "  accuracy: 0.13690105787181084\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 34884.007500339765\n",
            "  accuracy: 0.5943374197314653\n",
            "  accuracy: 0.5627463181912467\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 23141.247784628024\n",
            "  accuracy: 0.7304728546409808\n",
            "  accuracy: 0.6755859780128604\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 15073.23813212157\n",
            "  accuracy: 0.8451838879159369\n",
            "  accuracy: 0.750674133997096\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 10207.699771025225\n",
            "  accuracy: 0.9008756567425569\n",
            "  accuracy: 0.7678904791537026\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 7751.990728748206\n",
            "  accuracy: 0.9253356684179802\n",
            "  accuracy: 0.7751503837378138\n",
            "  best model from epoch 6\n",
            "epoch 7\n",
            "  epoch loss: 6622.862369486495\n",
            "  accuracy: 0.9164623467600701\n",
            "  accuracy: 0.7749429578925534\n",
            "  best model from epoch 6\n",
            "epoch 8\n",
            "  epoch loss: 5879.283340677779\n",
            "  accuracy: 0.914302393461763\n",
            "  accuracy: 0.7649865173200581\n",
            "  best model from epoch 6\n",
            "epoch 9\n",
            "  epoch loss: 5483.528795179387\n",
            "  accuracy: 0.923292469352014\n",
            "  accuracy: 0.7732835511304709\n",
            "  best model from epoch 6\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.7872386290148738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 0, the best taggging accuracy on DEV is 77.52% from epoch 6; this best model has a tagging accuracy of 78.72% on DEVTEST.   \n",
        "(In the cell outputs above, the first accuracy score in each epoch is the accuracy on TRAIN, and the second accuracy score in each epoch is the accuracy on DEV.)"
      ],
      "metadata": {
        "id": "LZLWcBugUhUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run model: w=1, all vocab"
      ],
      "metadata": {
        "id": "G9WvHFmLTzWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model: single hidden layer 128 with tanh nonlinearity, w=1, all vocab (random init)\n",
        "tagger_w1 = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],  # last layer is the output layer\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1.parameters(), lr=0.02)\n",
        "\n",
        "# train and eval\n",
        "epoch_losses, train_evals, dev_evals, devtest_eval = main_process(\n",
        "                                                          model=tagger_w1,\n",
        "                                                          name='tagger_w1',  # file name used for using checkpoint\n",
        "                                                          optimizer=sgd,\n",
        "                                                          criterion=nn.CrossEntropyLoss(),   # objective: log loss\n",
        "                                                          train_data=train_w1_allvocab,\n",
        "                                                          batch_size=1,\n",
        "                                                          shuffle=True,\n",
        "                                                          val_data=dev_w1_allvocab,\n",
        "                                                          test_data=devtest_w1_allvocab,\n",
        "                                                          max_epochs=20,\n",
        "                                                          early_stopping=3    # when dev eval doesn't improve for 3 consecutive epochs\n",
        "                                                      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiUuST0kddUY",
        "outputId": "67c9317f-5329-488c-e965-eb35703ebc68"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46249.25761397183\n",
            "  accuracy: 0.16351430239346176\n",
            "  accuracy: 0.15308027380211575\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 26344.955107870977\n",
            "  accuracy: 0.7582603619381203\n",
            "  accuracy: 0.716863721219664\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 12880.483928764414\n",
            "  accuracy: 0.8875072971395213\n",
            "  accuracy: 0.7660236465463597\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 7795.16006897964\n",
            "  accuracy: 0.925569176882662\n",
            "  accuracy: 0.791537025513379\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 5983.9946808077175\n",
            "  accuracy: 0.9436660828955049\n",
            "  accuracy: 0.7994192076332711\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 4676.601857609816\n",
            "  accuracy: 0.9543490951546993\n",
            "  accuracy: 0.8043974279195187\n",
            "  best model from epoch 6\n",
            "epoch 7\n",
            "  epoch loss: 3786.175114842786\n",
            "  accuracy: 0.9607705779334501\n",
            "  accuracy: 0.7940261356565028\n",
            "  best model from epoch 6\n",
            "epoch 8\n",
            "  epoch loss: 2924.863934574468\n",
            "  accuracy: 0.9640396964389959\n",
            "  accuracy: 0.7969300974901473\n",
            "  best model from epoch 6\n",
            "epoch 9\n",
            "  epoch loss: 2379.3492109089893\n",
            "  accuracy: 0.9624635143023934\n",
            "  accuracy: 0.7938187098112425\n",
            "  best model from epoch 6\n",
            "========= EARLY STOPPING =========\n",
            "load best model\n",
            "eval best model on devtest\n",
            "  accuracy: 0.8206510023712007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a window size of 1, the best taggging accuracy on DEV is 80.44% from epoch 6; this best model has a tagging accuracy of 82.07% on DEVTEST.   \n",
        "(In the cell outputs above, the first accuracy score in each epoch is the accuracy on TRAIN, and the second accuracy score in each epoch is the accuracy on DEV.)"
      ],
      "metadata": {
        "id": "sX6vUY0UVJWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 feature engineering"
      ],
      "metadata": {
        "id": "x4K_RderVlaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the errors the best tagger so far made on DEV\n",
        "\n",
        "# get a mask for correct predictions\n",
        "ycorrect_mask = torch.eq(\n",
        "    torch.argmax(tagger_w1(dev_w1_allvocab.wins), dim=1),  # pred\n",
        "    dev_w1_allvocab.tags_encoded  # true\n",
        ")\n",
        "\n",
        "# get center words with wrong pred\n",
        "yerror = dev_w1_allvocab.center_words[~ycorrect_mask]"
      ],
      "metadata": {
        "id": "iLGGBqhX7D7s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUQjOxQDgFly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XU_3DinqgZvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBQt-rx_gZsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sGR8wlCgZpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKFJS1VyYdYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJJGvEIb7Dqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model(dev_w0_allvocab.wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55d7N3ggcf5C",
        "outputId": "931b80bc-6574-4ecd-ee26-4b3017ba52c2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 14.9545,   7.8195,  -3.3007,  ...,   4.8590,   5.0649,  -7.5874],\n",
              "        [ -4.2738,  -6.1849,   4.4028,  ...,  -7.7645,   5.7791, -10.8850],\n",
              "        [ 11.2687,   4.8644,   1.0903,  ...,   0.6950,   9.4577, -15.1235],\n",
              "        ...,\n",
              "        [ -1.0855,  -1.5283,   4.3029,  ...,  -1.9557,   5.5269,  -6.1479],\n",
              "        [ -7.5095,   0.5310,   4.1376,  ...,   1.6492,   1.7842,  11.8887],\n",
              "        [  5.6387,   2.1828,   3.6610,  ...,  -0.0710,   8.9411, -10.3145]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger_w0(dev_w0_allvocab.wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4dXcKcJaKgP",
        "outputId": "a2f9bf92-c2a6-4cf3-d6ec-76af1dc1580c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 13.7580,   7.4531,  -0.7973,  ...,   3.3517,   0.4652,  -9.2348],\n",
              "        [ -4.9620,  -5.3379,   2.0708,  ...,  -6.7571,   7.6977,  -6.9138],\n",
              "        [  8.4014,   5.4101,   1.4895,  ...,  -0.2391,   7.4384, -13.2695],\n",
              "        ...,\n",
              "        [  0.6930,  -2.8784,   3.5722,  ...,  -2.9129,   5.8869,  -7.7423],\n",
              "        [ -7.5352,  -0.0978,   1.8020,  ...,   3.1513,   5.3963,  11.3258],\n",
              "        [  4.7839,   2.3918,   2.8345,  ...,  -0.3393,   9.4155,  -9.6965]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(best_model, dev_w0_allvocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFG2g3x6eBX_",
        "outputId": "338d210d-3218-4b28-b51b-f3acb6230ef5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  accuracy: 0.7718315702136486\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7718315702136486"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval(tagger_w0, dev_w0_allvocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvjKdpknefgA",
        "outputId": "44641d0d-398e-417a-f37f-422b6410bf82"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  accuracy: 0.7641568139390168\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7641568139390168"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftPNaZmJd4GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bddCbllGV114"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# - model state saving\n",
        "# - additional feature"
      ],
      "metadata": {
        "id": "pLm-FQECGOjC"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5n-kF2mZGOWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap encoded datasets in dataloaders: w = 0, all vocab\n",
        "train_w0_allvocab_dataloader = DataLoader(train_w0_allvocab, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# instantiate model w=0, all vocab\n",
        "tagger_w0 = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0.parameters(), lr=0.02)\n",
        "\n",
        "\n",
        "\n",
        "# train and eval\n",
        "max_epochs = 10\n",
        "\n",
        "epoch_losses = []\n",
        "train_accus = []\n",
        "dev_accus = []\n",
        "\n",
        "best_dev_accu = 0\n",
        "best_model = tagger_w0\n",
        "best_model_epoch = -1\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    # train\n",
        "    epoch_loss = train1epoch(\n",
        "                      model=tagger_w0,\n",
        "                      optimizer=sgd,\n",
        "                      criterion=nn.CrossEntropyLoss(),\n",
        "                      train_dataloader=train_w0_allvocab_dataloader\n",
        "                 )\n",
        "    epoch_losses.append(epoch_loss)\n",
        "\n",
        "    # eval on training set\n",
        "    train_accu = eval(model=tagger_w0, eval_data=train_w0_allvocab)\n",
        "    train_accus.append(train_accu)\n",
        "\n",
        "    # eval on dev set\n",
        "    dev_accu = eval(model=tagger_w0, eval_data=dev_w0_allvocab)\n",
        "    dev_accus.append(dev_accu)\n",
        "\n",
        "    # update best model\n",
        "    if dev_accu > best_dev_accu:\n",
        "        best_model = deepcopy(tagger_w0)  # save best_model so far\n",
        "        best_model_epoch = epoch  # update which epoch best_model is from\n",
        "        best_dev_accu = dev_accu  # update best_dev_accu\n",
        "        print(f'  best model from epoch {best_model_epoch+1}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "print(f'best model on devtest')\n",
        "devtest_accu = eval(model=best_model, eval_data=devtest_w0_allvocab)\n"
      ],
      "metadata": {
        "id": "kadbxDiu3S5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8584e8f5-b2ba-4524-af75-0b5f5ccf464e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46368.374962091446\n",
            "  accuracy: 0.11722124927028604\n",
            "  accuracy: 0.10371292263015972\n",
            "  best model from epoch 1\n",
            "epoch 2\n",
            "  epoch loss: 34184.30587075767\n",
            "  accuracy: 0.6823701109165208\n",
            "  accuracy: 0.6533914125700062\n",
            "  best model from epoch 2\n",
            "epoch 3\n",
            "  epoch loss: 18312.380577613396\n",
            "  accuracy: 0.8474022183304145\n",
            "  accuracy: 0.7307612528521054\n",
            "  best model from epoch 3\n",
            "epoch 4\n",
            "  epoch loss: 12563.383699736762\n",
            "  accuracy: 0.8927028604786924\n",
            "  accuracy: 0.7542003733665215\n",
            "  best model from epoch 4\n",
            "epoch 5\n",
            "  epoch loss: 9912.72281500377\n",
            "  accuracy: 0.9061879743140688\n",
            "  accuracy: 0.765608794855839\n",
            "  best model from epoch 5\n",
            "epoch 6\n",
            "  epoch loss: 8242.175429728086\n",
            "  accuracy: 0.9116170461179218\n",
            "  accuracy: 0.7703795892968264\n",
            "  best model from epoch 6\n",
            "epoch 7\n",
            "  epoch loss: 6970.244936785886\n",
            "  accuracy: 0.9151196730881495\n",
            "  accuracy: 0.7670607757726613\n",
            "epoch 8\n",
            "  epoch loss: 6286.66907957042\n",
            "  accuracy: 0.9092819614711033\n",
            "  accuracy: 0.7539929475212611\n",
            "epoch 9\n",
            "  epoch loss: 5822.152175433701\n",
            "  accuracy: 0.9217746643315821\n",
            "  accuracy: 0.7734909769757312\n",
            "  best model from epoch 9\n",
            "epoch 10\n",
            "  epoch loss: 5438.433563374492\n",
            "  accuracy: 0.9282545242265032\n",
            "  accuracy: 0.7768097904998963\n",
            "  best model from epoch 10\n",
            "best model on devtest\n",
            "  accuracy: 0.7915499029963354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hehixweh6_o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6dMXQUWvB65"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnDGpPze59pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZR3CH6cMHxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Stv1XXvcMHif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odYZficLTaGA"
      },
      "outputs": [],
      "source": [
        "# # wrap encoded datasets in dataloaders\n",
        "\n",
        "\n",
        "# # w = 0, all vocab\n",
        "# train_w0_allvocab_dataloader = DataLoader(train_w0_allvocab, batch_size=1, shuffle=True)\n",
        "# dev_w0_allvocab_dataloader = DataLoader(dev_w0_allvocab, batch_size=1, shuffle=True)\n",
        "# devtest_w0_allvocab_dataloader = DataLoader(devtest_w0_allvocab, batch_size=1, shuffle=True)\n",
        "\n",
        "# # w = 1, all vocab\n",
        "# train_w1_allvocab_dataloader = DataLoader(train_w1_allvocab, batch_size=1, shuffle=True)\n",
        "# dev_w1_allvocab_dataloader = DataLoader(dev_w1_allvocab, batch_size=1, shuffle=True)\n",
        "# devtest_w1_allvocab_dataloader = DataLoader(devtest_w1_allvocab, batch_size=1, shuffle=True)\n",
        "\n",
        "\n",
        "# # w = 0, pretrained 30k\n",
        "# train_w0_30k_dataloader = DataLoader(train_w0_30k, batch_size=1, shuffle=True)\n",
        "# dev_w0_30k_dataloader = DataLoader(dev_w0_30k, batch_size=1, shuffle=True)\n",
        "# devtest_w0_30k_dataloader = DataLoader(devtest_w0_30k, batch_size=1, shuffle=True)\n",
        "\n",
        "# # w = 1, pretrained 30k\n",
        "# train_w1_30k_dataloader = DataLoader(train_w1_30k, batch_size=1, shuffle=True)\n",
        "# dev_w1_30k_dataloader = DataLoader(dev_w1_30k, batch_size=1, shuffle=True)\n",
        "# devtest_w1_30k_dataloader = DataLoader(devtest_w1_30k, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPhEiJTBWYtC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pralt772rIZ1",
        "outputId": "b620ca0e-8d41-40ba-b3a7-7ffaf2d9a327"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 1.0425e-03, -5.8820e-03, -8.6644e-03,  ..., -7.9195e-03,\n",
              "         -3.7283e-03,  7.7972e-03],\n",
              "        [ 6.4962e-03, -6.2669e-03, -3.7780e-03,  ...,  2.8969e-03,\n",
              "          7.5352e-03, -4.0234e-03],\n",
              "        [-9.9592e-03, -4.3420e-03, -5.1301e-03,  ...,  4.6443e-04,\n",
              "         -6.4169e-03, -6.4338e-03],\n",
              "        ...,\n",
              "        [ 5.9787e-03,  1.8511e-03,  8.4176e-03,  ...,  6.5441e-03,\n",
              "         -5.4045e-03, -2.6694e-03],\n",
              "        [ 8.0650e-03, -3.3898e-03,  4.1751e-03,  ..., -3.1981e-04,\n",
              "          5.9205e-03,  3.9072e-03],\n",
              "        [-5.9091e-03, -6.2013e-03, -2.4727e-03,  ..., -3.6859e-03,\n",
              "          5.9371e-04, -7.3646e-05]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# # temb = nn.Embedding.from_pretrained(emb_pretrained, freeze=True)\n",
        "# temb = nn.Embedding(len(all_vocab), 50)\n",
        "# temb.weight.data.uniform_(-0.01, 0.01)\n",
        "# temb.weight\n",
        "# # print(temb.weight.shape)\n",
        "\n",
        "# curr_input = Xtrain_w1[0].unsqueeze(0)\n",
        "# # curr_input = Xtrain_w1[:1]\n",
        "# print(curr_input.shape)\n",
        "# concatemb = temb(curr_input).view((curr_input.shape[0], -1))\n",
        "# print(concatemb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6f38FF0tM_i",
        "outputId": "e187f0a9-603e-465e-e38d-44459e7acac7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 150])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# concatemb_flat = concatemb.view((concatemb.shape[0], -1))\n",
        "# concatemb_flat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuSIUt_dvCjk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pObIy1LMXCTu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT4woDB1wW5m",
        "outputId": "df6006c9-a5b6-4946-d8ef-8844ddd0bee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "  epoch loss: 46359.56888842583\n",
            "  train accu: 0.1514302393461763\n",
            "  dev accu: 0.1557768097904999\n",
            "epoch 2\n",
            "  epoch loss: 36511.22461539088\n",
            "  train accu: 0.6244016345592528\n",
            "  dev accu: 0.6050611906243518\n",
            "epoch 3\n",
            "  epoch loss: 20647.9830187872\n",
            "  train accu: 0.7897256275539989\n",
            "  dev accu: 0.7313835303878863\n",
            "epoch 4\n",
            "  epoch loss: 13142.984029729269\n",
            "  train accu: 0.883420899007589\n",
            "  dev accu: 0.751296411532877\n",
            "epoch 5\n",
            "  epoch loss: 9927.773577833897\n",
            "  train accu: 0.9056625802685347\n",
            "  dev accu: 0.7635345364032359\n",
            "epoch 6\n",
            "  epoch loss: 8590.880470508913\n",
            "  train accu: 0.9119089316987741\n",
            "  dev accu: 0.7624974071769343\n",
            "epoch 7\n",
            "  epoch loss: 7707.652470903413\n",
            "  train accu: 0.9249270286047869\n",
            "  dev accu: 0.7712092926778676\n",
            "epoch 8\n",
            "  epoch loss: 6816.784264039488\n",
            "  train accu: 0.9195563339171045\n",
            "  dev accu: 0.7660236465463597\n",
            "epoch 9\n",
            "  epoch loss: 6216.87197396993\n",
            "  train accu: 0.9262697022767076\n",
            "  dev accu: 0.7778469197261979\n",
            "epoch 10\n",
            "  epoch loss: 5744.520718641375\n",
            "  train accu: 0.9217746643315821\n",
            "  dev accu: 0.7745281062020327\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=0\n",
        "tagger_w0 = FeedForwardNN(w=0, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "# len(list(tagger_w0.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w0.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w0.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w0_allvocab_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w0(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w0.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w0(train_w0_allvocab.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w0_allvocab.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w0_allvocab.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w0.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w0(dev_w0_allvocab.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w0_allvocab.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w0_allvocab.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL_N_p7hXExc",
        "outputId": "ba79281e-1261-4346-85ce-76b951009d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "  epoch loss: 36041.978227217594\n",
            "  train accu: 0.5828371278458844\n",
            "  dev accu: 0.5718730553827007\n",
            "epoch 2\n",
            "  epoch loss: 16047.907971054286\n",
            "  train accu: 0.8486281377699941\n",
            "  dev accu: 0.7600082970338105\n",
            "epoch 3\n",
            "  epoch loss: 9350.579022063437\n",
            "  train accu: 0.9163455925277292\n",
            "  dev accu: 0.796100394109106\n",
            "epoch 4\n",
            "  epoch loss: 6425.25555942486\n",
            "  train accu: 0.9440163455925278\n",
            "  dev accu: 0.8048122796100394\n",
            "epoch 5\n",
            "  epoch loss: 4911.637813769848\n",
            "  train accu: 0.9469352014010508\n",
            "  dev accu: 0.7981746525617092\n",
            "epoch 6\n",
            "  epoch loss: 4130.616831530634\n",
            "  train accu: 0.9600700525394046\n",
            "  dev accu: 0.8025305953121759\n",
            "epoch 7\n",
            "  epoch loss: 3199.3462827699595\n",
            "  train accu: 0.9615878575598366\n",
            "  dev accu: 0.7987969300974902\n",
            "epoch 8\n",
            "  epoch loss: 2535.824724089276\n",
            "  train accu: 0.9718038528896672\n",
            "  dev accu: 0.8029454470026965\n",
            "epoch 9\n",
            "  epoch loss: 2146.7702989513787\n",
            "  train accu: 0.9804436660828955\n",
            "  dev accu: 0.7998340593237917\n",
            "epoch 10\n",
            "  epoch loss: 1670.3467638214952\n",
            "  train accu: 0.9821366024518389\n",
            "  dev accu: 0.8004563368595727\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=1\n",
        "tagger_w1 = FeedForwardNN(w=1, vocab_size=len(all_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=None, emb_freeze=False)\n",
        "# len(list(tagger_w1.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w1.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w1_allvocab_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w1(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w1.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w1(train_w1_allvocab.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w1_allvocab.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w1_allvocab.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w1.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w1(dev_w1_allvocab.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w1_allvocab.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w1_allvocab.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MADjYESipSlq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nrVRsTXpSij",
        "outputId": "54f2b426-73e8-484b-fa54-6fd4733fb672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "  epoch loss: 16369.005858714794\n",
            "  train accu: 0.8449503794512551\n",
            "  dev accu: 0.8344741754822651\n",
            "epoch 2\n",
            "  epoch loss: 9320.458782576461\n",
            "  train accu: 0.8655575014594279\n",
            "  dev accu: 0.8477494295789255\n",
            "epoch 3\n",
            "  epoch loss: 8422.924471781236\n",
            "  train accu: 0.8650321074138938\n",
            "  dev accu: 0.8471271520431446\n",
            "epoch 4\n",
            "  epoch loss: 7934.014341046579\n",
            "  train accu: 0.880502043199066\n",
            "  dev accu: 0.8574984443061605\n",
            "epoch 5\n",
            "  epoch loss: 7540.962700240924\n",
            "  train accu: 0.8785755983654407\n",
            "  dev accu: 0.8531425015556938\n",
            "epoch 6\n",
            "  epoch loss: 7192.139447728374\n",
            "  train accu: 0.8861646234676007\n",
            "  dev accu: 0.8514830947936113\n",
            "epoch 7\n",
            "  epoch loss: 6820.51915438598\n",
            "  train accu: 0.8892002335084647\n",
            "  dev accu: 0.8581207218419415\n",
            "epoch 8\n",
            "  epoch loss: 6506.891840068924\n",
            "  train accu: 0.8973730297723292\n",
            "  dev accu: 0.8657954781165733\n",
            "epoch 9\n",
            "  epoch loss: 6227.353143249253\n",
            "  train accu: 0.906713368359603\n",
            "  dev accu: 0.8684920141049575\n",
            "epoch 10\n",
            "  epoch loss: 5917.889376106414\n",
            "  train accu: 0.9095738470519557\n",
            "  dev accu: 0.8645509230450114\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=1, fixed pretrained embedding\n",
        "tagger_w1_fixedpretrained = FeedForwardNN(w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=True)\n",
        "# len(list(tagger_w1_fixedpretrained.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_fixedpretrained.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w1_fixedpretrained.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w1_30k_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w1_fixedpretrained(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w1_fixedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w1_fixedpretrained(train_w1_30k.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w1_30k.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w1_30k.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w1_fixedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w1_fixedpretrained(dev_w1_30k.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w1_30k.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w1_30k.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd3f3k4VuC2i",
        "outputId": "2a461b00-91d9-4e71-c778-98bb75f93978"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 8.0050e-03,  8.8390e-03, -7.6610e-03,  ...,  3.3940e-03,\n",
              "          4.0300e-04,  2.6620e-03],\n",
              "        [ 2.0712e-01, -3.1345e-02,  9.1379e-02,  ...,  1.5570e-01,\n",
              "         -6.7304e-02, -2.5445e-02],\n",
              "        [-3.2129e-01,  5.0717e-02,  2.1766e-01,  ..., -1.6058e-01,\n",
              "          5.7263e-02,  3.8416e-01],\n",
              "        ...,\n",
              "        [-8.6111e-01,  3.4025e-01,  1.6014e-01,  ...,  4.0112e-01,\n",
              "         -3.7185e-01, -4.0742e-01],\n",
              "        [-1.0521e-01, -2.7858e-01,  4.9961e-01,  ..., -2.5552e-01,\n",
              "         -2.2169e-01,  5.1709e-01],\n",
              "        [-2.3829e-01,  5.1810e-02,  3.3805e-01,  ...,  1.7722e-01,\n",
              "         -6.0529e-02, -1.6389e-01]])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagger_w1_fixedpretrained.emb.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6uSEDEguFNm",
        "outputId": "1de49d63-0c48-49f0-b4a5-3ecd41ac6b0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 8.0050e-03,  8.8390e-03, -7.6610e-03,  ...,  3.3940e-03,\n",
              "          4.0300e-04,  2.6620e-03],\n",
              "        [ 2.0712e-01, -3.1345e-02,  9.1379e-02,  ...,  1.5570e-01,\n",
              "         -6.7304e-02, -2.5445e-02],\n",
              "        [-3.2129e-01,  5.0717e-02,  2.1766e-01,  ..., -1.6058e-01,\n",
              "          5.7263e-02,  3.8416e-01],\n",
              "        ...,\n",
              "        [-8.6111e-01,  3.4025e-01,  1.6014e-01,  ...,  4.0112e-01,\n",
              "         -3.7185e-01, -4.0742e-01],\n",
              "        [-1.0521e-01, -2.7858e-01,  4.9961e-01,  ..., -2.5552e-01,\n",
              "         -2.2169e-01,  5.1709e-01],\n",
              "        [-2.3829e-01,  5.1810e-02,  3.3805e-01,  ...,  1.7722e-01,\n",
              "         -6.0529e-02, -1.6389e-01]])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emb_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYxdX37yuK2E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLD-5yGpuKtf",
        "outputId": "d55ddf33-36c1-4451-b741-ad7a532cf5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "  epoch loss: 14670.984842691923\n",
            "  train accu: 0.891009924109749\n",
            "  dev accu: 0.8550093341630367\n",
            "epoch 2\n",
            "  epoch loss: 6710.139597005856\n",
            "  train accu: 0.9273204903677759\n",
            "  dev accu: 0.8682845882596971\n",
            "epoch 3\n",
            "  epoch loss: 4973.363695524482\n",
            "  train accu: 0.9360186806771745\n",
            "  dev accu: 0.8624766645924082\n",
            "epoch 4\n",
            "  epoch loss: 4005.304377012074\n",
            "  train accu: 0.9389375364856976\n",
            "  dev accu: 0.8651732005807924\n",
            "epoch 5\n",
            "  epoch loss: 3292.6318096834343\n",
            "  train accu: 0.9558669001751313\n",
            "  dev accu: 0.8695291433312591\n",
            "epoch 6\n",
            "  epoch loss: 2835.4244742415253\n",
            "  train accu: 0.9660828955049621\n",
            "  dev accu: 0.8732628085459448\n",
            "epoch 7\n",
            "  epoch loss: 2353.4613265826006\n",
            "  train accu: 0.9709281961471103\n",
            "  dev accu: 0.8579132959966812\n",
            "epoch 8\n",
            "  epoch loss: 2089.4233974509316\n",
            "  train accu: 0.9732049036777584\n",
            "  dev accu: 0.8660029039618337\n",
            "epoch 9\n",
            "  epoch loss: 1726.4095906020575\n",
            "  train accu: 0.9776999416228839\n",
            "  dev accu: 0.8572910184609003\n",
            "epoch 10\n",
            "  epoch loss: 1580.652004197805\n",
            "  train accu: 0.9838295388207823\n",
            "  dev accu: 0.8574984443061605\n"
          ]
        }
      ],
      "source": [
        "# instantiate model w=1, fine-tuned pretrained embedding\n",
        "tagger_w1_tunedpretrained = FeedForwardNN(w=1, vocab_size=len(emb_pretrained_vocab), emb_dim=50,\n",
        "                          layer_sizes=[128, len(all_tags)],\n",
        "                          layer_acts=[nn.Tanh(), nn.Identity()],   # nn.CrossEntropyLoss() already includes softmax transformation\n",
        "                          pretrained_emb=emb_pretrained, emb_freeze=False)\n",
        "# len(list(tagger_w1_tunedpretrained.parameters()))\n",
        "\n",
        "\n",
        "# instantiate optimizer\n",
        "sgd = optim.SGD(tagger_w1_tunedpretrained.parameters(), lr=0.02)\n",
        "# set loss func\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "\n",
        "    # run 1 epoch\n",
        "    print(f'epoch {epoch+1}')\n",
        "\n",
        "    tagger_w1_tunedpretrained.train()  # turn on training mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for xtrain_batch, ytrain_batch in train_w1_30k_dataloader:\n",
        "\n",
        "        # print(xtrain_batch)\n",
        "        # print(ytrain_batch)\n",
        "\n",
        "        sgd.zero_grad()   # zero the gradient buffers\n",
        "        output = tagger_w1_tunedpretrained(xtrain_batch)\n",
        "        loss = criterion(output, ytrain_batch)\n",
        "        loss.backward()\n",
        "        sgd.step()   # does the update\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'  epoch loss: {epoch_loss}')\n",
        "\n",
        "\n",
        "    # eval on training set\n",
        "    tagger_w1_tunedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ytrain_pred = torch.argmax(tagger_w1_tunedpretrained(train_w1_30k.wins), dim=1)\n",
        "        ytrain_correct = torch.sum(torch.eq(ytrain_pred, train_w1_30k.tags_encoded)).item()\n",
        "        ytrain_total = len(train_w1_30k.tags_encoded)\n",
        "        ytrain_accu = ytrain_correct / ytrain_total\n",
        "    print(f'  train accu: {ytrain_accu}')\n",
        "\n",
        "    # eval on dev set\n",
        "    tagger_w1_tunedpretrained.eval()  # turn on eval mode\n",
        "    with torch.no_grad():  # turn off gradient calc to reduce memory consumption\n",
        "        ydev_pred = torch.argmax(tagger_w1_tunedpretrained(dev_w1_30k.wins), dim=1)\n",
        "        ydev_correct = torch.sum(torch.eq(ydev_pred, dev_w1_30k.tags_encoded)).item()\n",
        "        ydev_total = len(dev_w1_30k.tags_encoded)\n",
        "        ydev_accu = ydev_correct / ydev_total\n",
        "    print(f'  dev accu: {ydev_accu}')\n",
        "\n",
        "\n",
        "# eval best model on devtest set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaxhM2oSwWx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f85b5b-e0de-4161-e346-d7f3c0b3fea6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 5.5893e-02, -2.1001e-01, -2.9847e-02,  ..., -1.1455e-01,\n",
              "         -1.9788e-01,  2.1989e-01],\n",
              "        [ 2.8332e-01, -2.4272e-01, -6.2081e-03,  ...,  2.9925e-01,\n",
              "         -6.3570e-02,  5.4027e-02],\n",
              "        [-4.9772e-01, -1.5044e-01, -1.3602e-02,  ..., -2.1467e-01,\n",
              "         -2.3371e-03,  4.4186e-01],\n",
              "        ...,\n",
              "        [-8.6111e-01,  3.4025e-01,  1.6014e-01,  ...,  4.0112e-01,\n",
              "         -3.7185e-01, -4.0742e-01],\n",
              "        [-1.0521e-01, -2.7858e-01,  4.9961e-01,  ..., -2.5552e-01,\n",
              "         -2.2169e-01,  5.1709e-01],\n",
              "        [-4.5604e-03,  8.8334e-02,  1.7233e-01,  ..., -4.9501e-04,\n",
              "          4.8545e-02,  1.7906e-01]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "tagger_w1_tunedpretrained.emb.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXA7ipUrGJmE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-xiphYGJj4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHUOMYOzFVTG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMhURUk_ylZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idd1olUmEECE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LsEoAkEetfK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Gz1BGEetc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6SxzpQEfVJx5",
        "IW9XMJlvl-XT"
      ],
      "provenance": [],
      "mount_file_id": "19hhLki5hZJMZVBLonH_fmJtvco-Lldwg",
      "authorship_tag": "ABX9TyP7jwivttY+IbNyKjpPokVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}